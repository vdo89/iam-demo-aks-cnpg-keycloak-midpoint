name: 02 - Bootstrap ArgoCD, Addons, DB, Keycloak, midPoint

on:
  workflow_dispatch:
    inputs:
      LOCATION:
        description: 'Azure region'
        required: false
        default: 'westeurope'
      RESOURCE_GROUP:
        description: 'Resource Group name (output of TF)'
        required: true
        default: 'rwsdemo-rg'
      AKS_NAME:
        description: 'AKS cluster name (output of TF)'
        required: true
        default: 'rwsdemo-aks'
      STORAGE_ACCOUNT:
        description: 'Azure Storage Account name for CNPG backups (output of TF)'
        required: true
      NAMESPACE_IAM:
        description: 'Namespace for IAM stack (Keycloak + midPoint + DB)'
        required: false
        default: 'iam'

permissions:
  id-token: write
  contents: read

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Azure Login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AKS credentials
        uses: azure/aks-set-context@v4
        with:
          resource-group: ${{ inputs.RESOURCE_GROUP }}
          cluster-name: ${{ inputs.AKS_NAME }}

      - name: Create namespaces
        shell: bash
        run: |
          set -euo pipefail
          kubectl create ns argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ingress-nginx --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cert-manager --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cnpg-system --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ${{ inputs.NAMESPACE_IAM }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Argo CD (stable manifest)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          echo "Waiting for Argo CD core components to become ready..."
          for workload in deploy/argocd-server deploy/argocd-repo-server deploy/argocd-redis deploy/argocd-dex-server; do
            echo "Waiting for rollout of ${workload}"
            kubectl -n argocd rollout status "$workload" --timeout=300s
          done
          echo "Waiting for Argo CD application controller statefulset"
          kubectl -n argocd rollout status statefulset/argocd-application-controller --timeout=300s

      - name: Configure Argo CD repository credentials
        shell: bash
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          ARGOCD_REPO_USERNAME: ${{ secrets.ARGOCD_REPO_USERNAME }}
          ARGOCD_REPO_TOKEN: ${{ secrets.ARGOCD_REPO_TOKEN }}
        run: |
          set -euo pipefail

          REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          REPO_NAME="${GITHUB_REPOSITORY##*/}"
          if [ -z "${ARGOCD_REPO_USERNAME}" ] || [ -z "${ARGOCD_REPO_TOKEN}" ]; then
            echo "Secrets ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN not provided; checking if repository is public..."
            if curl -sS "https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}" | jq -e '.private == false' >/dev/null; then
              echo "Repository is public; skipping Argo CD repository secret."
              exit 0
            fi
            echo "Repository appears to be private. Provide ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN secrets so Argo CD can sync."
            exit 1
          fi

          sanitized=$(printf '%s' "${REPO_OWNER}-${REPO_NAME}" | tr '[:upper:]' '[:lower:]' | tr -c 'a-z0-9' '-' | sed 's/^-*//;s/-*$//')
          if [ -z "${sanitized}" ]; then
            sanitized="repo"
          fi
          SECRET_NAME="repo-${sanitized}"

          kubectl -n argocd create secret generic "${SECRET_NAME}" \
            --type=Opaque \
            --from-literal=url="https://github.com/${REPO_OWNER}/${REPO_NAME}" \
            --from-literal=username="${ARGOCD_REPO_USERNAME}" \
            --from-literal=password="${ARGOCD_REPO_TOKEN}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n argocd label secret "${SECRET_NAME}" \
            argocd.argoproj.io/secret-type=repository --overwrite
          echo "Configured Argo CD repository credentials in secret ${SECRET_NAME}"

      - name: Pre-install CloudNativePG CRDs (server-side apply)
        shell: bash
        run: |
          set -euo pipefail

          if ! command -v helm >/dev/null 2>&1; then
            echo "Helm not found on runner; installing Helm 3"
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          fi

          echo "Adding CloudNativePG Helm repository"
          helm repo add cloudnative-pg https://cloudnative-pg.github.io/charts --force-update >/dev/null 2>&1 || true

          echo "Updating CloudNativePG Helm repository index"
          for attempt in $(seq 1 5); do
            if helm repo update cloudnative-pg >/dev/null 2>&1; then
              echo "Helm repository index refreshed"
              break
            fi
            if [ "${attempt}" -eq 5 ]; then
              echo "Failed to update CloudNativePG Helm repository after ${attempt} attempts"
              exit 1
            fi
            echo "Helm repo update failed (attempt ${attempt}/5); retrying in 5 seconds"
            sleep 5
          done

          echo "Rendering CloudNativePG CRDs for version 0.22.1"
          if ! helm show crds cloudnative-pg/cloudnative-pg --version 0.22.1 > /tmp/cloudnative-pg-crds.yaml; then
            echo "helm show crds failed; aborting"
            exit 1
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "No CRDs were returned by 'helm show crds'; falling back to helm template rendering"
            helm template cloudnative-pg-crds cloudnative-pg/cloudnative-pg \
              --version 0.22.1 \
              --include-crds \
              --namespace cnpg-system > /tmp/cloudnative-pg-crds-rendered.yaml

            python3 <<'PY'
          import pathlib
          import sys

          rendered_path = pathlib.Path("/tmp/cloudnative-pg-crds-rendered.yaml")
          output_path = pathlib.Path("/tmp/cloudnative-pg-crds.yaml")

          if not rendered_path.exists():
              print("Rendered CRD manifest not found", file=sys.stderr)
              sys.exit(1)

          docs = []
          current = []
          for line in rendered_path.read_text().splitlines():
              if line.strip() == '---':
                  if current:
                      docs.append('\n'.join(current).strip())
                      current = []
                  continue
              current.append(line)
          if current:
              docs.append('\n'.join(current).strip())

          crd_docs = []
          for doc in docs:
              for line in doc.splitlines():
                  stripped = line.strip()
                  if stripped.startswith('kind:'):
                      if stripped.split(':', 1)[1].strip() == 'CustomResourceDefinition':
                          crd_docs.append(doc)
                      break

          if not crd_docs:
              print("No CustomResourceDefinition documents found in rendered template", file=sys.stderr)
              sys.exit(1)

          # Ensure each CRD document is separated by a YAML document marker on its
          # own line. Joining with "\n---\n" avoids concatenating the marker onto
          # the last line of the previous document when that document does not end
          # with a trailing newline (which would create invalid YAML like
          # "metadata:...---" and trigger kubectl parsing errors).
          output_path.write_text('\n---\n'.join(crd_docs) + '\n')

          PY
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "Failed to render CloudNativePG CRDs"
            ls -l /tmp/cloudnative-pg-crds*
            exit 1
          fi

          echo "Applying CloudNativePG CRDs with server-side apply"
          kubectl apply --server-side -f /tmp/cloudnative-pg-crds.yaml

          echo "Removing potential last-applied annotations that exceed Kubernetes limits"
          for crd in \
            backups.postgresql.cnpg.io \
            clusterimagecatalogs.postgresql.cnpg.io \
            clusters.postgresql.cnpg.io \
            imagecatalogs.postgresql.cnpg.io \
            poolers.postgresql.cnpg.io \
            scheduledbackups.postgresql.cnpg.io; do
            if kubectl get crd "${crd}" >/dev/null 2>&1; then
              kubectl annotate crd "${crd}" kubectl.kubernetes.io/last-applied-configuration- >/dev/null 2>&1 || true
            fi
          done

      - name: Sync addons via Argo (Ingress-NGINX, cert-manager, CNPG operator)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/root-apps.yaml | kubectl apply -f -
          echo "Waiting for Argo CD application 'addons' to be created and synced"
          addons_ready=0
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application addons >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "addons application is synced and healthy"
                addons_ready=1
                break
              fi
              echo "addons application status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "addons application not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          if [ "$addons_ready" -ne 1 ]; then
            echo "Timed out waiting for addons application to become healthy"
            kubectl -n argocd get application addons -o yaml || true
            exit 1
          fi
          kubectl -n argocd get application addons

      - name: Wait for cnpg-operator Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Argo CD application cnpg-operator to be created..."
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application cnpg-operator >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "cnpg-operator application is synced and healthy"
                exit 0
              fi
              echo "cnpg-operator status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "Application cnpg-operator not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          echo "Timed out waiting for cnpg-operator Argo CD application to become healthy"
          kubectl -n argocd get application cnpg-operator -o yaml || true
          exit 1

      - name: Wait for CNPG operator CRDs
        shell: bash
        run: |
          set -euo pipefail

          echo "Waiting for CloudNativePG CRDs to become available..."
          for attempt in $(seq 1 30); do
            if kubectl get crd clusters.postgresql.cnpg.io >/dev/null 2>&1; then
              if kubectl wait --for=condition=Established crd/clusters.postgresql.cnpg.io --timeout=60s; then
                if kubectl -n cnpg-system get deployment cnpg-cloudnative-pg >/dev/null 2>&1; then
                  if kubectl -n cnpg-system wait --for=condition=Available deployment/cnpg-cloudnative-pg --timeout=300s; then
                    echo "CloudNativePG operator deployment is available"
                    exit 0
                  else
                    echo "Deployment cnpg-cloudnative-pg exists but is not yet available (attempt ${attempt}/30)"
                  fi
                else
                  echo "Deployment cnpg-cloudnative-pg not found yet (attempt ${attempt}/30)"
                fi
              else
                echo "CRD clusters.postgresql.cnpg.io exists but is not yet established (attempt ${attempt}/30)"
              fi
            else
              echo "CRD clusters.postgresql.cnpg.io not found yet (attempt ${attempt}/30)"
            fi
            sleep 10
          done
          echo "Timed out waiting for CloudNativePG CRDs"
          exit 1

      - name: Wait for CNPG webhook service endpoints
        shell: bash
        run: |
          set -euo pipefail

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          echo "Ensuring cnpg-webhook-service has ready endpoints..."
          consecutive_ready=0
          for attempt in $(seq 1 45); do
            if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
              echo "cnpg-webhook-service not created yet (attempt ${attempt}/45)"
              consecutive_ready=0
            else
              collect_webhook_status
              log_webhook_status
              if [ -n "${WEBHOOK_READY_FROM_ENDPOINTS}" ] || [ -n "${WEBHOOK_READY_FROM_SLICES}" ]; then
                consecutive_ready=$((consecutive_ready + 1))
                echo "Ready endpoints observed (${consecutive_ready}/3 consecutive confirmations)"
                if [ "${consecutive_ready}" -ge 3 ]; then
                  echo "cnpg-webhook-service has ready endpoints that appear stable"
                  echo "Waiting a few seconds to let the Kubernetes API server register the webhook endpoints"
                  sleep 10
                  exit 0
                fi
                sleep 5
                continue
              fi

              echo "cnpg-webhook-service endpoints not ready yet (attempt ${attempt}/45)"
              consecutive_ready=0
            fi
            sleep 10
          done

          echo "Timed out waiting for cnpg-webhook-service endpoints"
          kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
          kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
          kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
          exit 1

      - name: Create CNPG secrets (DB users + superuser)
        shell: bash
        run: |
          set -euo pipefail

          ns="${{ inputs.NAMESPACE_IAM }}"

          if [ -z "${ns}" ]; then
            echo "NAMESPACE_IAM input must not be empty"
            exit 1
          fi

          ensure_secret_type() {
            local secret_name="$1"
            local expected_type="$2"

            if kubectl -n "${ns}" get secret "${secret_name}" >/dev/null 2>&1; then
              local current_type
              current_type=$(kubectl -n "${ns}" get secret "${secret_name}" -o jsonpath='{.type}')

              if [ "${current_type}" != "${expected_type}" ]; then
                echo "Secret ${secret_name} exists with type ${current_type}; recreating with type ${expected_type}"
                kubectl -n "${ns}" delete secret "${secret_name}" --wait=false --ignore-not-found
                echo "Waiting for secret ${secret_name} to be fully removed before recreating"
                for attempt in $(seq 1 12); do
                  if ! kubectl -n "${ns}" get secret "${secret_name}" >/dev/null 2>&1; then
                    echo "Secret ${secret_name} removed"
                    break
                  fi

                  if [ "${attempt}" -eq 12 ]; then
                    echo "Secret ${secret_name} still present after waiting; aborting"
                    exit 1
                  fi

                  sleep 5
                done
              fi
            fi
          }

          apply_basic_auth_secret() {
            local secret_name="$1"
            local username="$2"
            local password="$3"
            local password_source="$4"

            if [ -z "${password}" ]; then
              echo "ERROR: password value for secret ${secret_name} is empty (expected GitHub secret ${password_source})"
              exit 1
            fi

            ensure_secret_type "${secret_name}" "kubernetes.io/basic-auth"

            kubectl -n "${ns}" create secret generic "${secret_name}" \
              --type=kubernetes.io/basic-auth \
              --from-literal=username="${username}" \
              --from-literal=password="${password}" \
              --dry-run=client -o yaml | kubectl apply -f -

            echo "Secret ${secret_name} ensured with type kubernetes.io/basic-auth"
          }

          apply_basic_auth_secret "cnpg-superuser" "postgres" "${{ secrets.POSTGRES_SUPERUSER_PASSWORD }}" "POSTGRES_SUPERUSER_PASSWORD"
          apply_basic_auth_secret "keycloak-db-app" "keycloak" "${{ secrets.KEYCLOAK_DB_PASSWORD }}" "KEYCLOAK_DB_PASSWORD"
          apply_basic_auth_secret "midpoint-db-app" "midpoint" "${{ secrets.MIDPOINT_DB_PASSWORD }}" "MIDPOINT_DB_PASSWORD"

      - name: Create Azure Blob secret for CNPG backups (connection string or key)
        shell: bash
        env:
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail

          AZURE_STORAGE_ACCOUNT_RAW="${AZURE_STORAGE_ACCOUNT:-}"
          AZURE_STORAGE_ACCOUNT="$(python3 -c "import os; print(os.environ.get('AZURE_STORAGE_ACCOUNT', '').strip())")"

          AZURE_STORAGE_KEY_RAW="${AZURE_STORAGE_KEY:-}"
          AZURE_STORAGE_KEY="$(python3 -c "import os; print(os.environ.get('AZURE_STORAGE_KEY', '').strip())")"

          if [ -z "${AZURE_STORAGE_ACCOUNT}" ]; then
            echo "AZURE_STORAGE_ACCOUNT input must not be empty"
            exit 1
          fi

          if [ -z "${AZURE_STORAGE_KEY}" ]; then
            echo "AZURE_STORAGE_KEY secret is required for demo backup. Add it in repo secrets."
            exit 1
          fi

          if [ "${AZURE_STORAGE_ACCOUNT_RAW}" != "${AZURE_STORAGE_ACCOUNT}" ]; then
            echo "Trimmed leading/trailing whitespace from AZURE_STORAGE_ACCOUNT input"
          fi

          if [ "${AZURE_STORAGE_KEY_RAW}" != "${AZURE_STORAGE_KEY}" ]; then
            echo "Trimmed leading/trailing whitespace from AZURE_STORAGE_KEY secret value"
          fi

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic cnpg-azure-backup \
            --from-literal=AZURE_STORAGE_ACCOUNT="${AZURE_STORAGE_ACCOUNT}" \
            --from-literal=AZURE_STORAGE_KEY="${AZURE_STORAGE_KEY}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Purge existing CNPG Azure backup prefix
        shell: bash
        env:
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
          RESOURCE_GROUP: ${{ inputs.RESOURCE_GROUP }}
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        run: |
          set -euo pipefail

          container="cnpg-backups"
          prefix="iam-db"

          AZURE_STORAGE_ACCOUNT_RAW="${AZURE_STORAGE_ACCOUNT:-}"
          AZURE_STORAGE_ACCOUNT="$(python3 -c "import os; print(os.environ.get('AZURE_STORAGE_ACCOUNT', '').strip())")"

          AZURE_STORAGE_KEY_RAW="${AZURE_STORAGE_KEY:-}"
          _AZ_PARSE_CODE=$'import os\nimport urllib.parse\n\nvalue = os.environ.get("AZURE_STORAGE_KEY", "")\ntrimmed = value.strip()\nif not trimmed:\n    key_type = "empty"\n    account_key = ""\n    account_name = ""\n    sas_token = ""\nelse:\n    lowered = trimmed.lower()\n    account_key = ""\n    account_name = ""\n    sas_token = ""\n    key_type = ""\n    if (("accountkey=" in lowered) or ("sharedaccesssignature=" in lowered)) and (\n        ("accountname=" in lowered) or ("blobendpoint=" in lowered) or ("defaultendpointsprotocol=" in lowered)\n    ):\n        parts = {}\n        for part in trimmed.split(";"):\n            if not part or "=" not in part:\n                continue\n            key, val = part.split("=", 1)\n            parts[key.strip().lower()] = val.strip()\n        account_key = parts.get("accountkey", "")\n        account_name = parts.get("accountname", "")\n        shared_sig = parts.get("sharedaccesssignature", "")\n        if shared_sig:\n            sas_token = shared_sig.lstrip(" ?")\n        if account_key:\n            key_type = "connection_string"\n        elif shared_sig:\n            key_type = "connection_string_sas"\n        else:\n            key_type = "connection_string"\n    if not key_type or key_type == "empty":\n        candidate = trimmed\n        if candidate.lower().startswith("https://"):\n            try:\n                parsed = urllib.parse.urlparse(candidate)\n            except Exception:\n                parsed = None\n            if parsed:\n                if parsed.query:\n                    candidate = parsed.query\n                elif parsed.fragment:\n                    candidate = parsed.fragment\n                elif parsed.path:\n                    candidate = parsed.path\n        candidate = candidate.lstrip("/?")\n        lowered_candidate = candidate.lower()\n        if "sig=" in lowered_candidate and ("se=" in lowered_candidate or "expiry=" in lowered_candidate):\n            sas_token = candidate\n            key_type = "sas"\n    if not key_type:\n        key_type = "account_key"\n\nprint(trimmed)\nprint(key_type)\nprint(account_key)\nprint(account_name)\nprint(sas_token)\n'
          mapfile -t _AZ_STORAGE_INFO < <(python3 -c "$_AZ_PARSE_CODE")
          unset _AZ_PARSE_CODE
          AZURE_STORAGE_KEY="${_AZ_STORAGE_INFO[0]-}"
          storage_key_type="${_AZ_STORAGE_INFO[1]-}"
          connection_account_key="${_AZ_STORAGE_INFO[2]-}"
          connection_account_name="${_AZ_STORAGE_INFO[3]-}"
          sanitized_sas_token="${_AZ_STORAGE_INFO[4]-}"
          unset _AZ_STORAGE_INFO

          if [ -z "${AZURE_STORAGE_ACCOUNT}" ]; then
            if [ -n "${connection_account_name}" ]; then
              AZURE_STORAGE_ACCOUNT="${connection_account_name}"
              echo "Derived storage account name ${AZURE_STORAGE_ACCOUNT} from AZURE_STORAGE_KEY secret"
            else
              echo "STORAGE_ACCOUNT input must not be empty"
              exit 1
            fi
          fi

          if [ "${AZURE_STORAGE_ACCOUNT_RAW}" != "${AZURE_STORAGE_ACCOUNT}" ]; then
            echo "Trimmed leading/trailing whitespace from AZURE_STORAGE_ACCOUNT input"
          fi

          RESOURCE_GROUP_RAW="${RESOURCE_GROUP:-}"
          RESOURCE_GROUP="$(python3 -c "import os; print(os.environ.get('RESOURCE_GROUP', '').strip())")"

          if [ -n "${RESOURCE_GROUP_RAW}" ] && [ "${RESOURCE_GROUP_RAW}" != "${RESOURCE_GROUP}" ]; then
            echo "Trimmed leading/trailing whitespace from RESOURCE_GROUP input"
          fi

          NAMESPACE_IAM_RAW="${NAMESPACE_IAM:-}"
          NAMESPACE_IAM="$(python3 -c "import os; print(os.environ.get('NAMESPACE_IAM', '').strip())")"

          if [ -n "${NAMESPACE_IAM_RAW}" ] && [ "${NAMESPACE_IAM_RAW}" != "${NAMESPACE_IAM}" ]; then
            echo "Trimmed leading/trailing whitespace from NAMESPACE_IAM input"
          fi

          user_supplied_key=1
          if [ -z "${AZURE_STORAGE_KEY}" ]; then
            user_supplied_key=0
          fi

          if [ "${AZURE_STORAGE_KEY_RAW}" != "${AZURE_STORAGE_KEY}" ] && [ "${user_supplied_key}" -eq 1 ]; then
            echo "Trimmed leading/trailing whitespace from AZURE_STORAGE_KEY secret"
          fi

          if [ -n "${connection_account_name}" ] && [ "${AZURE_STORAGE_ACCOUNT,,}" != "${connection_account_name,,}" ]; then
            echo "WARNING: STORAGE_ACCOUNT (${AZURE_STORAGE_ACCOUNT}) does not match AccountName (${connection_account_name}) parsed from AZURE_STORAGE_KEY"
          fi

          account_key_for_cli=""
          if [ -n "${connection_account_key}" ]; then
            account_key_for_cli="${connection_account_key}"
          elif [ "${storage_key_type}" = "account_key" ] && [ -n "${AZURE_STORAGE_KEY}" ]; then
            account_key_for_cli="${AZURE_STORAGE_KEY}"
          fi

          account_key_source="provided"
          if [ "${user_supplied_key}" -eq 0 ]; then
            account_key_source="missing"
            echo "AZURE_STORAGE_KEY secret not provided; attempting to use Azure AD authentication or retrieve a key dynamically."
          fi

          resource_group_available=0
          if [ -n "${RESOURCE_GROUP}" ]; then
            resource_group_available=1
          fi

          attempt_storage_auth_methods() {
            local methods=("$@")
            for method in "${methods[@]}"; do
              local current_args=()
              local method_description=""
              case "${method}" in
                connection-string)
                  current_args=("--connection-string" "${AZURE_STORAGE_KEY}")
                  method_description="connection string"
                  ;;
                sas-token)
                  current_args=("--account-name" "${AZURE_STORAGE_ACCOUNT}" "--sas-token" "${sanitized_sas_token}")
                  method_description="SAS token"
                  ;;
                account-key)
                  if [ -z "${account_key_for_cli}" ]; then
                    continue
                  fi
                  current_args=("--account-name" "${AZURE_STORAGE_ACCOUNT}" "--account-key" "${account_key_for_cli}")
                  if [ "${account_key_source}" = "fetched" ]; then
                    method_description="account key (fetched from Azure)"
                  else
                    method_description="account key"
                  fi
                  ;;
                login)
                  current_args=("--account-name" "${AZURE_STORAGE_ACCOUNT}" "--auth-mode" "login")
                  method_description="Azure AD login"
                  ;;
                *)
                  continue
                  ;;
              esac

              local err_file
              err_file=$(mktemp)
              set +e
              local exists_output
              exists_output=$(az storage container exists --name "${container}" "${current_args[@]}" --query exists -o tsv 2>"${err_file}")
              local status=$?
              set -e

              if [ "${status}" -eq 0 ]; then
                rm -f "${err_file}"
                az_auth_args=("${current_args[@]}")
                selected_method="${method}"
                container_exists="${exists_output:-false}"
                return 0
              fi

              local err_output=""
              if [ -s "${err_file}" ]; then
                err_output="$(cat "${err_file}")"
              fi
              rm -f "${err_file}"

              echo "Azure Storage authentication attempt using ${method_description} credentials failed (exit code ${status})."
              if [ -n "${err_output}" ]; then
                echo "${err_output}"
              fi

              if [ "${method}" = "account-key" ] && [ "${account_key_source}" != "fetched" ] && grep -qi 'account key may be not valid' <<<"${err_output}"; then
                echo "The provided AZURE_STORAGE_KEY value does not appear to be a valid storage account key."
              fi

              if [ "${method}" = "login" ] && grep -qi 'AuthorizationPermissionMismatch' <<<"${err_output}"; then
                echo "Azure AD authentication succeeded but lacks data-plane permissions. Assign the Storage Blob Data Contributor role to the workflow principal or supply a valid storage access key."
              fi
            done

            return 1
          }

          fetch_storage_account_key() {
            if [ "${resource_group_available}" -ne 1 ]; then
              return 1
            fi
            local err_file
            err_file=$(mktemp)
            echo "Attempting to retrieve a storage account key for ${AZURE_STORAGE_ACCOUNT} from resource group ${RESOURCE_GROUP}"
            set +e
            local fetched_key
            fetched_key=$(az storage account keys list --account-name "${AZURE_STORAGE_ACCOUNT}" --resource-group "${RESOURCE_GROUP}" --query '[0].value' -o tsv 2>"${err_file}")
            local status=$?
            set -e
            local err_output=""
            if [ "${status}" -ne 0 ]; then
              if [ -s "${err_file}" ]; then
                err_output="$(cat "${err_file}")"
              fi
              rm -f "${err_file}"
              echo "Failed to retrieve storage account key from Azure (exit code ${status})."
              if [ -n "${err_output}" ]; then
                echo "${err_output}"
              fi
              return 1
            fi
            rm -f "${err_file}"
            fetched_key="$(printf '%s' "${fetched_key}" | tr -d '\r\n')"
            if [ -z "${fetched_key}" ]; then
              echo "Azure CLI returned an empty storage account key for ${AZURE_STORAGE_ACCOUNT}."
              return 1
            fi
            account_key_for_cli="${fetched_key}"
            AZURE_STORAGE_KEY="${fetched_key}"
            account_key_source="fetched"
            echo "Retrieved storage account key for ${AZURE_STORAGE_ACCOUNT} via Azure CLI."
            if [ -n "${NAMESPACE_IAM:-}" ]; then
              echo "Updating cnpg-azure-backup secret with refreshed credentials in namespace ${NAMESPACE_IAM}"
              kubectl -n "${NAMESPACE_IAM}" create secret generic cnpg-azure-backup \
                --from-literal=AZURE_STORAGE_ACCOUNT="${AZURE_STORAGE_ACCOUNT}" \
                --from-literal=AZURE_STORAGE_KEY="${AZURE_STORAGE_KEY}" \
                --dry-run=client -o yaml | kubectl apply -f -
            else
              echo "WARNING: NAMESPACE_IAM environment variable is empty; skipping cnpg-azure-backup secret refresh."
            fi
            return 0
          }

          declare -a primary_methods=()
          if [ "${storage_key_type}" = "connection_string" ] || [ "${storage_key_type}" = "connection_string_sas" ]; then
            primary_methods+=("connection-string")
          fi
          if [ -n "${sanitized_sas_token}" ] && [ -n "${AZURE_STORAGE_ACCOUNT}" ]; then
            primary_methods+=("sas-token")
          fi
          if [ -n "${account_key_for_cli}" ] && [ -n "${AZURE_STORAGE_ACCOUNT}" ]; then
            primary_methods+=("account-key")
          fi

          declare -a az_auth_args=()
          selected_method=""
          container_exists="false"

          if [ "${#primary_methods[@]}" -gt 0 ]; then
            attempt_storage_auth_methods "${primary_methods[@]}" || true
          fi

          if [ -z "${selected_method}" ] && [ "${resource_group_available}" -eq 1 ]; then
            if fetch_storage_account_key; then
              attempt_storage_auth_methods "account-key" || true
            elif [ "${user_supplied_key}" -eq 0 ]; then
              echo "Unable to automatically retrieve a storage account key. Provide AZURE_STORAGE_KEY or grant the workflow principal Storage Blob Data Contributor access."
            fi
          fi

          if [ -z "${selected_method}" ]; then
            attempt_storage_auth_methods "login" || true
          fi

          if [ -z "${selected_method}" ]; then
            echo "Unable to authenticate to Azure Storage with the provided credentials or Azure AD login."
            exit 1
          fi

          case "${selected_method}" in
            connection-string)
              echo "Authenticated to Azure Storage using connection string credentials."
              ;;
            sas-token)
              echo "Authenticated to Azure Storage using SAS token credentials."
              ;;
            account-key)
              if [ "${account_key_source}" = "fetched" ]; then
                echo "Authenticated to Azure Storage using a storage account key retrieved via Azure CLI."
              else
                echo "Authenticated to Azure Storage using storage account key credentials."
              fi
              ;;
            login)
              echo "Authenticated to Azure Storage using Azure AD login credentials."
              ;;
          esac

          handle_az_storage_failure() {
            local status="$1"
            local err_file="$2"
            local context="$3"
            local stdout_capture="${4:-}"
            local err_output=""
            if [ -s "${err_file}" ]; then
              err_output="$(cat "${err_file}")"
              echo "${err_output}"
            fi
            rm -f "${err_file}"
            if [ -n "${stdout_capture}" ]; then
              echo "${stdout_capture}"
            fi

            local combined_output="${err_output}${stdout_capture}"
            if grep -qi 'account key may be not valid' <<<"${combined_output}"; then
              echo "Azure CLI reported invalid storage credentials while ${context}. Ensure AZURE_STORAGE_KEY contains a valid storage account key, SAS token, or connection string."
            fi

            if [[ " ${az_auth_args[*]} " == *" --auth-mode login "* ]] && grep -qi 'AuthorizationPermissionMismatch' <<<"${combined_output}"; then
              echo "Azure AD authentication for storage lacks the necessary data-plane permissions. Assign the Storage Blob Data Contributor role to the workflow principal or provide a valid storage access key."
            fi

            exit "${status}"
          }

          run_az_storage_command() {
            local capture="$1"
            local context="$2"
            shift 2
            local err_file output status
            err_file=$(mktemp)
            if [ "${capture}" = "capture" ]; then
              set +e
              output=$(az storage "$@" "${az_auth_args[@]}" 2>"${err_file}")
              status=$?
              set -e
              if [ "${status}" -ne 0 ]; then
                handle_az_storage_failure "${status}" "${err_file}" "${context}" "${output}"
              fi
              rm -f "${err_file}"
              printf '%s' "${output}"
            else
              set +e
              az storage "$@" "${az_auth_args[@]}" 2>"${err_file}"
              status=$?
              set -e
              if [ "${status}" -ne 0 ]; then
                handle_az_storage_failure "${status}" "${err_file}" "${context}" ""
              fi
              rm -f "${err_file}"
            fi
          }

          container_exists_normalized="$(printf '%s' "${container_exists}" | tr '[:upper:]' '[:lower:]')"
          if [ "${container_exists_normalized}" != "true" ]; then
            echo "Azure container ${container} not found; creating it"
            run_az_storage_command no-capture "creating container ${container}" container create --name "${container}" --public-access off
          fi

          echo "Checking for leftover WAL/archive objects under ${container}/${prefix}"
          existing_count="$(run_az_storage_command capture "listing blobs under ${prefix}/" blob list --container-name "${container}" --prefix "${prefix}/" --query 'length(@)' -o tsv)"
          existing_count="$(printf '%s' "${existing_count}" | tr -d '\r\n')"
          if [ -z "${existing_count}" ]; then
            existing_count=0
          fi

          if [ "${existing_count}" -eq 0 ]; then
            echo "Azure backup prefix ${prefix}/ is already empty"
            exit 0
          fi

          echo "Deleting ${existing_count} Azure Blob object(s) under ${prefix}/ before bootstrapping CNPG"
          run_az_storage_command no-capture "deleting blobs under ${prefix}/" blob delete-batch --source "${container}" --pattern "${prefix}/*" --no-progress

          echo "Verifying Azure backup prefix ${prefix}/ is now empty"
          remaining_count="$(run_az_storage_command capture "verifying blob cleanup" blob list --container-name "${container}" --prefix "${prefix}/" --query 'length(@)' -o tsv)"
          remaining_count="$(printf '%s' "${remaining_count}" | tr -d '\r\n')"
          if [ -z "${remaining_count}" ]; then
            remaining_count=0
          fi

          if [ "${remaining_count}" -ne 0 ]; then
            echo "Failed to purge Azure backup prefix ${prefix}/ (still ${remaining_count} object(s) present)"
            exit 1
          fi

          echo "Azure backup prefix ${prefix}/ successfully emptied"

      - name: Validate CNPG prerequisites
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          ns="${NAMESPACE_IAM}"
          echo "Checking required database secrets exist in namespace ${ns}"
          missing=0
          for secret in cnpg-superuser keycloak-db-app midpoint-db-app cnpg-azure-backup; do
            if ! kubectl -n "${ns}" get secret "${secret}" >/dev/null 2>&1; then
              echo "ERROR: secret ${secret} is missing from namespace ${ns}"
              missing=1
            fi
          done

          if [ "${missing}" -ne 0 ]; then
            echo "One or more required secrets are missing; aborting."
            exit 1
          fi

          check_secret_key() {
            local secret="$1"
            local key="$2"
            local value
            value=$(kubectl -n "${ns}" get secret "${secret}" -o jsonpath="{.data.${key}}" 2>/dev/null || true)
            if [ -z "${value}" ]; then
              echo "ERROR: secret ${secret} does not contain key ${key}"
              missing=1
            fi
          }

          check_secret_key cnpg-superuser username
          check_secret_key cnpg-superuser password
          check_secret_key keycloak-db-app username
          check_secret_key keycloak-db-app password
          check_secret_key midpoint-db-app username
          check_secret_key midpoint-db-app password
          check_secret_key cnpg-azure-backup AZURE_STORAGE_ACCOUNT
          check_secret_key cnpg-azure-backup AZURE_STORAGE_KEY

          if [ "${missing}" -ne 0 ]; then
            echo "Secret validation failed; aborting."
            exit 1
          fi

          echo "Confirming CNPG operator deployment readiness"
          kubectl -n cnpg-system get deployment cnpg-cloudnative-pg
          if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
            echo "WARNING: cnpg-cloudnative-pg deployment not yet available"
          fi

          echo "Inspecting CNPG webhook service endpoints"
          if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
            echo "ERROR: cnpg-webhook-service not found in cnpg-system namespace"
            exit 1
          fi

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          collect_webhook_status
          log_webhook_status
          if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
            echo "ERROR: cnpg-webhook-service currently has no ready endpoints"
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            exit 1
          fi

          ready_endpoints="${WEBHOOK_READY_FROM_ENDPOINTS:-${WEBHOOK_READY_FROM_SLICES}}"

          echo "cnpg-webhook-service ready endpoints: ${ready_endpoints:-<unknown>}"

      - name: Apply CNPG cluster (iam-db)
        env:
          STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          manifest="$(mktemp)"
          trap 'rm -f "${manifest}"' EXIT
          sed "s/{{STORAGE_ACCOUNT}}/${STORAGE_ACCOUNT}/g" k8s/apps/cnpg/cluster.yaml > "${manifest}"

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          is_webhook_unavailable_error() {
            local message="$1"
            if grep -qi 'no endpoints available for service "cnpg-webhook-service"' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook "mcluster.cnpg.io"' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            return 1
          }

          max_attempts=8
          success=0
          for attempt in $(seq 1 "${max_attempts}"); do
            echo "Applying CNPG cluster manifest (attempt ${attempt}/${max_attempts})"

            prereqs_ready=1
            if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
              echo "cnpg-cloudnative-pg deployment not yet available"
              prereqs_ready=0
            fi

            if ! kubectl -n cnpg-system wait --for=condition=Ready pod -l app.kubernetes.io/name=cloudnative-pg --timeout=180s; then
              echo "CNPG operator pods are not yet in Ready state"
              prereqs_ready=0
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service has no ready endpoints before attempt ${attempt}"
              prereqs_ready=0
            fi

            if [ "${prereqs_ready}" -ne 1 ]; then
              echo "CNPG operator prerequisites are not ready, waiting before retrying"
              sleep 20
              continue
            fi

            echo "Running server-side dry run to verify CNPG webhooks are reachable"
            if dry_run_output=$(kubectl apply --dry-run=server -f "${manifest}" 2>&1); then
              echo "${dry_run_output}"
            else
              echo "${dry_run_output}"
              if is_webhook_unavailable_error "${dry_run_output}"; then
                echo "Server-side dry run indicates CNPG webhooks are unavailable; retrying after delay"
                sleep 20
                continue
              fi
              echo "Server-side dry run failed for an unexpected reason; showing diagnostics"
              kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
              kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
              kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
              kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
              sleep 20
              continue
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service endpoints disappeared after dry run; waiting before retry"
              sleep 20
              continue
            fi

            if apply_output=$(kubectl apply -f "${manifest}" 2>&1); then
              echo "${apply_output}"
              echo "CNPG cluster manifest applied successfully"
              success=1
              break
            else
              echo "${apply_output}"
              if is_webhook_unavailable_error "${apply_output}"; then
                echo "kubectl apply failed because CNPG webhooks were unavailable; retrying after gathering diagnostics"
              else
                echo "kubectl apply failed on attempt ${attempt}; showing CNPG operator diagnostics"
              fi
            fi

            kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
            kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            sleep 20
          done

          if [ "${success:-0}" -ne 1 ]; then
            echo "Failed to apply CNPG cluster manifest after ${max_attempts} attempts"
            exit 1
          fi

      - name: Ensure midPoint database and role exist
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          ns="${NAMESPACE_IAM}"
          if [ -z "${ns}" ]; then
            echo "NAMESPACE_IAM input must not be empty"
            exit 1
          fi

          echo "Waiting for CloudNativePG cluster iam-db resource to be created"
          cluster_found=0
          for attempt in $(seq 1 30); do
            if kubectl -n "${ns}" get cluster iam-db >/dev/null 2>&1; then
              cluster_found=1
              break
            fi
            echo "Cluster iam-db not found yet in namespace ${ns} (attempt ${attempt}/30)"
            sleep 10
          done

          if [ "${cluster_found}" -ne 1 ]; then
            echo "Timed out waiting for Cluster iam-db resource to be created"
            kubectl -n "${ns}" get cluster iam-db -o yaml || true
            exit 1
          fi

          echo "Waiting for CloudNativePG cluster iam-db Ready condition"
          if ! kubectl -n "${ns}" wait --for=condition=Ready cluster/iam-db --timeout=600s; then
            echo "Cluster iam-db did not reach Ready condition within timeout"
            kubectl -n "${ns}" get cluster iam-db -o yaml || true
            kubectl -n "${ns}" get pods -l cnpg.io/cluster=iam-db -o wide || true
            kubectl -n "${ns}" describe cluster iam-db || true
            exit 1
          fi

          DB_READY_FROM_ENDPOINTS=""
          DB_NOT_READY_FROM_ENDPOINTS=""
          DB_READY_FROM_SLICES=""
          DB_NOT_READY_FROM_SLICES=""

          collect_db_endpoint_status() {
            DB_READY_FROM_ENDPOINTS=""
            DB_NOT_READY_FROM_ENDPOINTS=""
            DB_READY_FROM_SLICES=""
            DB_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n "${ns}" get endpoints iam-db-rw -o json 2>/dev/null); then
              DB_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              DB_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n "${ns}" get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=iam-db-rw -o json 2>/dev/null); then
              DB_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              DB_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_db_endpoint_status() {
            echo "iam-db-rw ready IPs (Endpoints): ${DB_READY_FROM_ENDPOINTS:-<none>}"
            echo "iam-db-rw notReady IPs (Endpoints): ${DB_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "iam-db-rw ready IPs (EndpointSlices): ${DB_READY_FROM_SLICES:-<none>}"
            echo "iam-db-rw notReady IPs (EndpointSlices): ${DB_NOT_READY_FROM_SLICES:-<none>}"
          }

          echo "Waiting for iam-db read/write service endpoints in namespace ${ns}"
          endpoints=""
          consecutive_ready=0
          max_attempts=45
          for attempt in $(seq 1 "${max_attempts}"); do
            collect_db_endpoint_status
            log_db_endpoint_status

            if [ -n "${DB_READY_FROM_ENDPOINTS}" ] || [ -n "${DB_READY_FROM_SLICES}" ]; then
              endpoints="${DB_READY_FROM_ENDPOINTS:-${DB_READY_FROM_SLICES}}"
              consecutive_ready=$((consecutive_ready + 1))
              echo "iam-db-rw has ready endpoints (${consecutive_ready}/3 consecutive confirmations)"
              if [ "${consecutive_ready}" -ge 3 ]; then
                break
              fi
              sleep 5
              continue
            fi

            echo "iam-db-rw service has no ready endpoints yet (attempt ${attempt}/${max_attempts})"
            consecutive_ready=0
            sleep 10
          done

          if [ -z "${endpoints}" ]; then
            echo "Timed out waiting for iam-db-rw service endpoints"
            kubectl -n "${ns}" get svc iam-db-rw -o yaml || true
            kubectl -n "${ns}" get endpoints iam-db-rw -o yaml || true
            kubectl -n "${ns}" get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=iam-db-rw -o yaml || true
            kubectl -n "${ns}" get pods -l cnpg.io/cluster=iam-db -o wide || true
            kubectl -n "${ns}" describe cluster iam-db || true
            exit 1
          fi

          echo "iam-db-rw endpoints: ${endpoints}"

          job_name="midpoint-db-bootstrap"
          kubectl -n "${ns}" delete job "${job_name}" --ignore-not-found

          manifest="$(mktemp)"
          cleanup_manifest() {
            rm -f "${manifest}"
          }
          trap cleanup_manifest EXIT

          cat <<'YAML' | sed 's/^          //' >"${manifest}"
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: midpoint-db-bootstrap
            namespace: ${NS}
          spec:
            backoffLimit: 3
            template:
              spec:
                restartPolicy: Never
                containers:
                  - name: psql
                    image: ghcr.io/cloudnative-pg/postgresql:16.4
                    env:
                      - name: PGPASSWORD
                        valueFrom:
                          secretKeyRef:
                            name: cnpg-superuser
                            key: password
                      - name: MIDPOINT_DB_USER
                        valueFrom:
                          secretKeyRef:
                            name: midpoint-db-app
                            key: username
                      - name: MIDPOINT_DB_PASSWORD
                        valueFrom:
                          secretKeyRef:
                            name: midpoint-db-app
                            key: password
                      - name: DB_HOST
                        value: iam-db-rw.${NS}.svc.cluster.local
                    command:
                      - bash
                      - -lc
                      - |
                        set -euo pipefail
                        psql -h "${DB_HOST}" -U postgres -v ON_ERROR_STOP=1 \
                          --set=mp_user="${MIDPOINT_DB_USER}" \
                          --set=mp_password="${MIDPOINT_DB_PASSWORD}" <<'SQL'
                        DO $do$
                        DECLARE
                          role_name text := :'mp_user';
                          role_password text := :'mp_password';
                        BEGIN
                          IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = role_name) THEN
                            EXECUTE format('CREATE ROLE %I LOGIN PASSWORD %L', role_name, role_password);
                          ELSE
                            EXECUTE format('ALTER ROLE %I PASSWORD %L', role_name, role_password);
                            EXECUTE format('ALTER ROLE %I LOGIN', role_name);
                          END IF;
                        END
                          $do$;

                          DO $do$
                          DECLARE
                            role_name text := :'mp_user';
                          BEGIN
                            IF NOT EXISTS (SELECT 1 FROM pg_database WHERE datname = 'midpoint') THEN
                              EXECUTE format('CREATE DATABASE %I OWNER %I', 'midpoint', role_name);
                            ELSE
                              EXECUTE format('ALTER DATABASE %I OWNER TO %I', 'midpoint', role_name);
                            END IF;
                          END
                          $do$;
                          SQL
          YAML
          export NS="${ns}"
          envsubst '$NS' < "${manifest}" | kubectl apply -f -

          if ! kubectl -n "${ns}" wait --for=condition=Complete job/"${job_name}" --timeout=240s; then
            echo "midPoint database bootstrap job did not complete successfully"
            kubectl -n "${ns}" logs job/"${job_name}" || true
            kubectl -n "${ns}" describe job "${job_name}" || true
            kubectl -n "${ns}" get pods -l job-name="${job_name}" -o wide || true
            exit 1
          fi

          kubectl -n "${ns}" logs job/"${job_name}" || true
          kubectl -n "${ns}" delete job "${job_name}" --ignore-not-found

          kubectl -n "${ns}" wait cluster/iam-db --for=condition=Ready --timeout=600s || true

      - name: Install Keycloak Operator (CRDs + operator Deployment)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloaks.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/kubernetes.yml

      - name: Prepare midPoint config and admin secret
        shell: bash
        env:
          MIDPOINT_ADMIN_PASSWORD: ${{ secrets.MIDPOINT_ADMIN_PASSWORD }}
        run: |
          set -euo pipefail

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic midpoint-admin \
            --from-literal=password="$MIDPOINT_ADMIN_PASSWORD" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n ${{ inputs.NAMESPACE_IAM }} create configmap midpoint-config \
            --from-file=config.xml=k8s/apps/midpoint/config.xml \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Wait for Keycloak operator CRDs
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Keycloak CRDs to become available..."
          for crd in keycloaks.k8s.keycloak.org keycloakrealmimports.k8s.keycloak.org; do
            kubectl wait --for=condition=Established crd/${crd} --timeout=300s
          done
          for ns in keycloak-system keycloak default; do
            if kubectl -n "$ns" get deployment keycloak-operator >/dev/null 2>&1; then
              kubectl -n "$ns" wait --for=condition=Available deployment/keycloak-operator --timeout=300s || true
              break
            fi
          done

      - name: Create Argo CD application for iam apps (Keycloak + midPoint)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/apps.yaml | kubectl apply -f -

      - name: Wait for iam apps Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          echo "Ensuring Argo CD application 'apps' has been created before monitoring status"
          app_found=0
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application apps >/dev/null 2>&1; then
              app_found=1
              echo "Argo CD application 'apps' detected"
              break
            fi
            echo "Application 'apps' not found yet (attempt ${attempt}/60)"
            sleep 10
          done

          if [ "${app_found}" -ne 1 ]; then
            echo "Timed out waiting for Argo CD application 'apps' to be created"
            kubectl -n argocd get applications || true
            exit 1
          fi

          echo "Waiting for Argo CD application 'apps' to report Synced/Healthy status"
          diagnostics_dumped=0
          for attempt in $(seq 1 90); do
            sync_status=$(kubectl -n argocd get application apps -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
            health_status=$(kubectl -n argocd get application apps -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
            operation_phase=$(kubectl -n argocd get application apps -o jsonpath='{.status.operationState.phase}' 2>/dev/null || echo "")

            echo "apps status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} operation=${operation_phase:-<unknown>} (attempt ${attempt}/90)"

            if [ "${sync_status}" = "Synced" ]; then
              if [ "${health_status}" = "Healthy" ]; then
                echo "apps application is synced and healthy"
                kubectl -n argocd get application apps
                exit 0
              fi
              if [ "${health_status}" = "Unknown" ] && [ "${operation_phase}" = "Succeeded" ]; then
                echo "apps application is synced; health reported as Unknown but last operation succeeded. Proceeding."
                kubectl -n argocd get application apps
                exit 0
              fi
            fi

            if [ "${operation_phase}" = "Failed" ] && [ "${diagnostics_dumped}" -eq 0 ]; then
              echo "Latest Argo CD sync operation failed. Dumping application manifest for troubleshooting."
              kubectl -n argocd get application apps -o yaml || true
              diagnostics_dumped=1
            fi

            if [ "${health_status}" = "Degraded" ] && [ "${diagnostics_dumped}" -eq 0 ]; then
              echo "apps application health is Degraded. Dumping application manifest for troubleshooting."
              kubectl -n argocd get application apps -o yaml || true
              diagnostics_dumped=1
            fi

            sleep 10
          done

          echo "Timed out waiting for Argo CD application 'apps' to become synced and healthy"
          kubectl -n argocd get application apps -o yaml || true
          exit 1

      - name: Show ingress endpoints (if available)
        shell: bash
        run: |
          set -euo pipefail
          echo "Ingress-NGINX service:"
          kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide || true
          echo "Keycloak service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc rws-keycloak -o wide || true
          echo "midPoint service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc midpoint -o wide || true
