name: 02 - Bootstrap ArgoCD, Addons, DB, Keycloak, midPoint

on:
  workflow_dispatch:
    inputs:
      LOCATION:
        description: 'Azure region'
        required: false
        default: 'westeurope'
      RESOURCE_GROUP:
        description: 'Resource Group name (output of TF)'
        required: true
        default: 'rwsdemo-rg'
      AKS_NAME:
        description: 'AKS cluster name (output of TF)'
        required: true
        default: 'rwsdemo-aks'
      STORAGE_ACCOUNT:
        description: 'Azure Storage Account name for CNPG backups (output of TF)'
        required: true
      NAMESPACE_IAM:
        description: 'Namespace for IAM stack (Keycloak + midPoint + DB)'
        required: false
        default: 'iam'

permissions:
  id-token: write
  contents: read

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Azure Login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AKS credentials
        uses: azure/aks-set-context@v4
        with:
          resource-group: ${{ inputs.RESOURCE_GROUP }}
          cluster-name: ${{ inputs.AKS_NAME }}

      - name: Create namespaces
        shell: bash
        run: |
          set -euo pipefail
          kubectl create ns argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ingress-nginx --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cert-manager --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cnpg-system --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ${{ inputs.NAMESPACE_IAM }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Argo CD (stable manifest)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          echo "Waiting for Argo CD core components to become ready..."
          for workload in deploy/argocd-server deploy/argocd-repo-server deploy/argocd-redis deploy/argocd-dex-server; do
            echo "Waiting for rollout of ${workload}"
            kubectl -n argocd rollout status "$workload" --timeout=300s
          done
          echo "Waiting for Argo CD application controller statefulset"
          kubectl -n argocd rollout status statefulset/argocd-application-controller --timeout=300s

      - name: Configure Argo CD repository credentials
        shell: bash
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          ARGOCD_REPO_USERNAME: ${{ secrets.ARGOCD_REPO_USERNAME }}
          ARGOCD_REPO_TOKEN: ${{ secrets.ARGOCD_REPO_TOKEN }}
        run: |
          set -euo pipefail

          REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          REPO_NAME="${GITHUB_REPOSITORY##*/}"
          if [ -z "${ARGOCD_REPO_USERNAME}" ] || [ -z "${ARGOCD_REPO_TOKEN}" ]; then
            echo "Secrets ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN not provided; checking if repository is public..."
            if curl -sS "https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}" | jq -e '.private == false' >/dev/null; then
              echo "Repository is public; skipping Argo CD repository secret."
              exit 0
            fi
            echo "Repository appears to be private. Provide ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN secrets so Argo CD can sync."
            exit 1
          fi

          sanitized=$(printf '%s' "${REPO_OWNER}-${REPO_NAME}" | tr '[:upper:]' '[:lower:]' | tr -c 'a-z0-9' '-' | sed 's/^-*//;s/-*$//')
          if [ -z "${sanitized}" ]; then
            sanitized="repo"
          fi
          SECRET_NAME="repo-${sanitized}"

          kubectl -n argocd create secret generic "${SECRET_NAME}" \
            --type=Opaque \
            --from-literal=url="https://github.com/${REPO_OWNER}/${REPO_NAME}" \
            --from-literal=username="${ARGOCD_REPO_USERNAME}" \
            --from-literal=password="${ARGOCD_REPO_TOKEN}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n argocd label secret "${SECRET_NAME}" \
            argocd.argoproj.io/secret-type=repository --overwrite
          echo "Configured Argo CD repository credentials in secret ${SECRET_NAME}"

      - name: Pre-install CloudNativePG CRDs (server-side apply)
        shell: bash
        run: |
          set -euo pipefail

          if ! command -v helm >/dev/null 2>&1; then
            echo "Helm not found on runner; installing Helm 3"
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          fi

          echo "Adding CloudNativePG Helm repository"
          helm repo add cloudnative-pg https://cloudnative-pg.github.io/charts --force-update >/dev/null 2>&1 || true

          echo "Updating CloudNativePG Helm repository index"
          for attempt in $(seq 1 5); do
            if helm repo update cloudnative-pg >/dev/null 2>&1; then
              echo "Helm repository index refreshed"
              break
            fi
            if [ "${attempt}" -eq 5 ]; then
              echo "Failed to update CloudNativePG Helm repository after ${attempt} attempts"
              exit 1
            fi
            echo "Helm repo update failed (attempt ${attempt}/5); retrying in 5 seconds"
            sleep 5
          done

          echo "Rendering CloudNativePG CRDs for version 0.22.1"
          if ! helm show crds cloudnative-pg/cloudnative-pg --version 0.22.1 > /tmp/cloudnative-pg-crds.yaml; then
            echo "helm show crds failed; aborting"
            exit 1
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "No CRDs were returned by 'helm show crds'; falling back to helm template rendering"
            helm template cloudnative-pg-crds cloudnative-pg/cloudnative-pg \
              --version 0.22.1 \
              --include-crds \
              --namespace cnpg-system > /tmp/cloudnative-pg-crds-rendered.yaml

              python3 <<'PY'
              import pathlib
              import sys

              rendered_path = pathlib.Path("/tmp/cloudnative-pg-crds-rendered.yaml")
              output_path = pathlib.Path("/tmp/cloudnative-pg-crds.yaml")

              if not rendered_path.exists():
                  print("Rendered CRD manifest not found", file=sys.stderr)
                  sys.exit(1)

              docs = []
              current = []
              for line in rendered_path.read_text().splitlines():
                  if line.strip() == '---':
                      if current:
                          docs.append('\n'.join(current).strip())
                          current = []
                      continue
                  current.append(line)
              if current:
                  docs.append('\n'.join(current).strip())

              crd_docs = []
              for doc in docs:
                  for line in doc.splitlines():
                      stripped = line.strip()
                      if stripped.startswith('kind:'):
                          if stripped.split(':', 1)[1].strip() == 'CustomResourceDefinition':
                              crd_docs.append(doc)
                          break

              if not crd_docs:
                  print("No CustomResourceDefinition documents found in rendered template", file=sys.stderr)
                  sys.exit(1)

              output_path.write_text('---\n'.join(crd_docs) + '\n')
              PY
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "Failed to render CloudNativePG CRDs"
            ls -l /tmp/cloudnative-pg-crds*
            exit 1
          fi

          echo "Applying CloudNativePG CRDs with server-side apply"
          kubectl apply --server-side -f /tmp/cloudnative-pg-crds.yaml

          echo "Removing potential last-applied annotations that exceed Kubernetes limits"
          for crd in \
            backups.postgresql.cnpg.io \
            clusterimagecatalogs.postgresql.cnpg.io \
            clusters.postgresql.cnpg.io \
            imagecatalogs.postgresql.cnpg.io \
            poolers.postgresql.cnpg.io \
            scheduledbackups.postgresql.cnpg.io; do
            if kubectl get crd "${crd}" >/dev/null 2>&1; then
              kubectl annotate crd "${crd}" kubectl.kubernetes.io/last-applied-configuration- >/dev/null 2>&1 || true
            fi
          done

      - name: Sync addons via Argo (Ingress-NGINX, cert-manager, CNPG operator)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/root-apps.yaml | kubectl apply -f -
          echo "Waiting for Argo CD application 'addons' to be created and synced"
          addons_ready=0
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application addons >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "addons application is synced and healthy"
                addons_ready=1
                break
              fi
              echo "addons application status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "addons application not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          if [ "$addons_ready" -ne 1 ]; then
            echo "Timed out waiting for addons application to become healthy"
            kubectl -n argocd get application addons -o yaml || true
            exit 1
          fi
          kubectl -n argocd get application addons

      - name: Wait for cnpg-operator Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Argo CD application cnpg-operator to be created..."
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application cnpg-operator >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "cnpg-operator application is synced and healthy"
                exit 0
              fi
              echo "cnpg-operator status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "Application cnpg-operator not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          echo "Timed out waiting for cnpg-operator Argo CD application to become healthy"
          kubectl -n argocd get application cnpg-operator -o yaml || true
          exit 1

      - name: Wait for CNPG operator CRDs
        shell: bash
        run: |
          set -euo pipefail

          echo "Waiting for CloudNativePG CRDs to become available..."
          for attempt in $(seq 1 30); do
            if kubectl get crd clusters.postgresql.cnpg.io >/dev/null 2>&1; then
              if kubectl wait --for=condition=Established crd/clusters.postgresql.cnpg.io --timeout=60s; then
                if kubectl -n cnpg-system get deployment cnpg-cloudnative-pg >/dev/null 2>&1; then
                  if kubectl -n cnpg-system wait --for=condition=Available deployment/cnpg-cloudnative-pg --timeout=300s; then
                    echo "CloudNativePG operator deployment is available"
                    exit 0
                  else
                    echo "Deployment cnpg-cloudnative-pg exists but is not yet available (attempt ${attempt}/30)"
                  fi
                else
                  echo "Deployment cnpg-cloudnative-pg not found yet (attempt ${attempt}/30)"
                fi
              else
                echo "CRD clusters.postgresql.cnpg.io exists but is not yet established (attempt ${attempt}/30)"
              fi
            else
              echo "CRD clusters.postgresql.cnpg.io not found yet (attempt ${attempt}/30)"
            fi
            sleep 10
          done
          echo "Timed out waiting for CloudNativePG CRDs"
          exit 1

      - name: Wait for CNPG webhook service endpoints
        shell: bash
        run: |
          set -euo pipefail

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          echo "Ensuring cnpg-webhook-service has ready endpoints..."
          consecutive_ready=0
          for attempt in $(seq 1 45); do
            if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
              echo "cnpg-webhook-service not created yet (attempt ${attempt}/45)"
              consecutive_ready=0
            else
              collect_webhook_status
              log_webhook_status
              if [ -n "${WEBHOOK_READY_FROM_ENDPOINTS}" ] || [ -n "${WEBHOOK_READY_FROM_SLICES}" ]; then
                consecutive_ready=$((consecutive_ready + 1))
                echo "Ready endpoints observed (${consecutive_ready}/3 consecutive confirmations)"
                if [ "${consecutive_ready}" -ge 3 ]; then
                  echo "cnpg-webhook-service has ready endpoints that appear stable"
                  echo "Waiting a few seconds to let the Kubernetes API server register the webhook endpoints"
                  sleep 10
                  exit 0
                fi
                sleep 5
                continue
              fi

              echo "cnpg-webhook-service endpoints not ready yet (attempt ${attempt}/45)"
              consecutive_ready=0
            fi
            sleep 10
          done

          echo "Timed out waiting for cnpg-webhook-service endpoints"
          kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
          kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
          kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
          exit 1

      - name: Create CNPG secrets (DB users + superuser)
        shell: bash
        run: |
          set -euo pipefail

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic cnpg-superuser \
            --from-literal=password='${{ secrets.POSTGRES_SUPERUSER_PASSWORD }}' \
            --dry-run=client -o yaml | kubectl apply -f -

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic keycloak-db-app \
            --from-literal=username='keycloak' \
            --from-literal=password='${{ secrets.KEYCLOAK_DB_PASSWORD }}' \
            --dry-run=client -o yaml | kubectl apply -f -

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic midpoint-db-app \
            --from-literal=username='midpoint' \
            --from-literal=password='${{ secrets.MIDPOINT_DB_PASSWORD }}' \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Create Azure Blob secret for CNPG backups (connection string or key)
        shell: bash
        env:
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail

          if [ -z "$AZURE_STORAGE_KEY" ]; then
            echo "AZURE_STORAGE_KEY secret is required for demo backup. Add it in repo secrets."
            exit 1
          fi
          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic cnpg-azure-backup \
            --from-literal=AZURE_STORAGE_ACCOUNT="$AZURE_STORAGE_ACCOUNT" \
            --from-literal=AZURE_STORAGE_KEY="$AZURE_STORAGE_KEY" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Validate CNPG prerequisites
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          ns="${NAMESPACE_IAM}"
          echo "Checking required database secrets exist in namespace ${ns}"
          missing=0
          for secret in cnpg-superuser keycloak-db-app midpoint-db-app cnpg-azure-backup; do
            if ! kubectl -n "${ns}" get secret "${secret}" >/dev/null 2>&1; then
              echo "ERROR: secret ${secret} is missing from namespace ${ns}"
              missing=1
            fi
          done

          if [ "${missing}" -ne 0 ]; then
            echo "One or more required secrets are missing; aborting."
            exit 1
          fi

          check_secret_key() {
            local secret="$1"
            local key="$2"
            local value
            value=$(kubectl -n "${ns}" get secret "${secret}" -o jsonpath="{.data.${key}}" 2>/dev/null || true)
            if [ -z "${value}" ]; then
              echo "ERROR: secret ${secret} does not contain key ${key}"
              missing=1
            fi
          }

          check_secret_key cnpg-superuser password
          check_secret_key keycloak-db-app username
          check_secret_key keycloak-db-app password
          check_secret_key midpoint-db-app username
          check_secret_key midpoint-db-app password
          check_secret_key cnpg-azure-backup AZURE_STORAGE_ACCOUNT
          check_secret_key cnpg-azure-backup AZURE_STORAGE_KEY

          if [ "${missing}" -ne 0 ]; then
            echo "Secret validation failed; aborting."
            exit 1
          fi

          echo "Confirming CNPG operator deployment readiness"
          kubectl -n cnpg-system get deployment cnpg-cloudnative-pg
          if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
            echo "WARNING: cnpg-cloudnative-pg deployment not yet available"
          fi

          echo "Inspecting CNPG webhook service endpoints"
          if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
            echo "ERROR: cnpg-webhook-service not found in cnpg-system namespace"
            exit 1
          fi

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          collect_webhook_status
          log_webhook_status
          if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
            echo "ERROR: cnpg-webhook-service currently has no ready endpoints"
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            exit 1
          fi

          ready_endpoints="${WEBHOOK_READY_FROM_ENDPOINTS:-${WEBHOOK_READY_FROM_SLICES}}"

          echo "cnpg-webhook-service ready endpoints: ${ready_endpoints:-<unknown>}"

      - name: Apply CNPG cluster (iam-db)
        env:
          STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          manifest="$(mktemp)"
          trap 'rm -f "${manifest}"' EXIT
          sed "s/{{STORAGE_ACCOUNT}}/${STORAGE_ACCOUNT}/g" k8s/apps/cnpg/cluster.yaml > "${manifest}"

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          is_webhook_unavailable_error() {
            local message="$1"
            if grep -qi 'no endpoints available for service "cnpg-webhook-service"' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook "mcluster.cnpg.io"' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            return 1
          }

          max_attempts=8
          success=0
          for attempt in $(seq 1 "${max_attempts}"); do
            echo "Applying CNPG cluster manifest (attempt ${attempt}/${max_attempts})"

            prereqs_ready=1
            if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
              echo "cnpg-cloudnative-pg deployment not yet available"
              prereqs_ready=0
            fi

            if ! kubectl -n cnpg-system wait --for=condition=Ready pod -l app.kubernetes.io/name=cloudnative-pg --timeout=180s; then
              echo "CNPG operator pods are not yet in Ready state"
              prereqs_ready=0
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service has no ready endpoints before attempt ${attempt}"
              prereqs_ready=0
            fi

            if [ "${prereqs_ready}" -ne 1 ]; then
              echo "CNPG operator prerequisites are not ready, waiting before retrying"
              sleep 20
              continue
            fi

            echo "Running server-side dry run to verify CNPG webhooks are reachable"
            if dry_run_output=$(kubectl apply --dry-run=server -f "${manifest}" 2>&1); then
              echo "${dry_run_output}"
            else
              echo "${dry_run_output}"
              if is_webhook_unavailable_error "${dry_run_output}"; then
                echo "Server-side dry run indicates CNPG webhooks are unavailable; retrying after delay"
                sleep 20
                continue
              fi
              echo "Server-side dry run failed for an unexpected reason; showing diagnostics"
              kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
              kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
              kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
              kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
              sleep 20
              continue
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service endpoints disappeared after dry run; waiting before retry"
              sleep 20
              continue
            fi

            if apply_output=$(kubectl apply -f "${manifest}" 2>&1); then
              echo "${apply_output}"
              echo "CNPG cluster manifest applied successfully"
              success=1
              break
            else
              echo "${apply_output}"
              if is_webhook_unavailable_error "${apply_output}"; then
                echo "kubectl apply failed because CNPG webhooks were unavailable; retrying after gathering diagnostics"
              else
                echo "kubectl apply failed on attempt ${attempt}; showing CNPG operator diagnostics"
              fi
            fi

            kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
            kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            sleep 20
          done

          if [ "${success:-0}" -ne 1 ]; then
            echo "Failed to apply CNPG cluster manifest after ${max_attempts} attempts"
            exit 1
          fi

          kubectl -n "${NAMESPACE_IAM}" wait cluster/iam-db --for=condition=Ready --timeout=600s || true

      - name: Install Keycloak Operator (CRDs + operator Deployment)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloaks.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/kubernetes.yml

      - name: Prepare midPoint config and admin secret
        shell: bash
        env:
          MIDPOINT_ADMIN_PASSWORD: ${{ secrets.MIDPOINT_ADMIN_PASSWORD }}
        run: |
          set -euo pipefail

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic midpoint-admin \
            --from-literal=password="$MIDPOINT_ADMIN_PASSWORD" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n ${{ inputs.NAMESPACE_IAM }} create configmap midpoint-config \
            --from-file=config.xml=k8s/apps/midpoint/config.xml \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Wait for Keycloak operator CRDs
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Keycloak CRDs to become available..."
          for crd in keycloaks.k8s.keycloak.org keycloakrealmimports.k8s.keycloak.org; do
            kubectl wait --for=condition=Established crd/${crd} --timeout=300s
          done
          for ns in keycloak-system keycloak default; do
            if kubectl -n "$ns" get deployment keycloak-operator >/dev/null 2>&1; then
              kubectl -n "$ns" wait --for=condition=Available deployment/keycloak-operator --timeout=300s || true
              break
            fi
          done

      - name: Create Argo CD application for iam apps (Keycloak + midPoint)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/apps.yaml | kubectl apply -f -

      - name: Wait for iam apps Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n argocd wait --for=condition=Synced applications/apps --timeout=600s
          kubectl -n argocd wait --for=condition=Healthy applications/apps --timeout=600s || true

      - name: Show ingress endpoints (if available)
        shell: bash
        run: |
          set -euo pipefail
          echo "Ingress-NGINX service:"
          kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide || true
          echo "Keycloak service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc rws-keycloak -o wide || true
          echo "midPoint service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc midpoint -o wide || true
