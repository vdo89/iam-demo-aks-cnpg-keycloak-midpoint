name: 02 - Bootstrap ArgoCD, Addons, DB, Keycloak, midPoint

on:
  workflow_dispatch:
    inputs:
      LOCATION:
        description: 'Azure region'
        required: false
        default: 'westeurope'
      RESOURCE_GROUP:
        description: 'Resource Group name (output of TF)'
        required: true
        default: 'rwsdemo-rg'
      AKS_NAME:
        description: 'AKS cluster name (output of TF)'
        required: true
        default: 'rwsdemo-aks'
      STORAGE_ACCOUNT:
        description: 'Azure Storage Account name for CNPG backups (output of TF)'
        required: true
      NAMESPACE_IAM:
        description: 'Namespace for IAM stack (Keycloak + midPoint + DB)'
        required: false
        default: 'iam'

permissions:
  id-token: write
  contents: read

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Azure Login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AKS credentials
        uses: azure/aks-set-context@v4
        with:
          resource-group: ${{ inputs.RESOURCE_GROUP }}
          cluster-name: ${{ inputs.AKS_NAME }}

      - name: Create namespaces
        shell: bash
        run: |
          set -euo pipefail
          kubectl create ns argocd --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ingress-nginx --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cert-manager --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns cnpg-system --dry-run=client -o yaml | kubectl apply -f -
          kubectl create ns ${{ inputs.NAMESPACE_IAM }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Install Argo CD (stable manifest)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          echo "Waiting for Argo CD core components to become ready..."
          for workload in deploy/argocd-server deploy/argocd-repo-server deploy/argocd-redis deploy/argocd-dex-server; do
            echo "Waiting for rollout of ${workload}"
            kubectl -n argocd rollout status "$workload" --timeout=300s
          done
          echo "Waiting for Argo CD application controller statefulset"
          kubectl -n argocd rollout status statefulset/argocd-application-controller --timeout=300s

      - name: Configure Argo CD repository credentials
        shell: bash
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          ARGOCD_REPO_USERNAME: ${{ secrets.ARGOCD_REPO_USERNAME }}
          ARGOCD_REPO_TOKEN: ${{ secrets.ARGOCD_REPO_TOKEN }}
        run: |
          set -euo pipefail

          REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          REPO_NAME="${GITHUB_REPOSITORY##*/}"
          if [ -z "${ARGOCD_REPO_USERNAME}" ] || [ -z "${ARGOCD_REPO_TOKEN}" ]; then
            echo "Secrets ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN not provided; checking if repository is public..."
            if curl -sS "https://api.github.com/repos/${REPO_OWNER}/${REPO_NAME}" | jq -e '.private == false' >/dev/null; then
              echo "Repository is public; skipping Argo CD repository secret."
              exit 0
            fi
            echo "Repository appears to be private. Provide ARGOCD_REPO_USERNAME and ARGOCD_REPO_TOKEN secrets so Argo CD can sync."
            exit 1
          fi

          sanitized=$(printf '%s' "${REPO_OWNER}-${REPO_NAME}" | tr '[:upper:]' '[:lower:]' | tr -c 'a-z0-9' '-' | sed 's/^-*//;s/-*$//')
          if [ -z "${sanitized}" ]; then
            sanitized="repo"
          fi
          SECRET_NAME="repo-${sanitized}"

          kubectl -n argocd create secret generic "${SECRET_NAME}" \
            --type=Opaque \
            --from-literal=url="https://github.com/${REPO_OWNER}/${REPO_NAME}" \
            --from-literal=username="${ARGOCD_REPO_USERNAME}" \
            --from-literal=password="${ARGOCD_REPO_TOKEN}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n argocd label secret "${SECRET_NAME}" \
            argocd.argoproj.io/secret-type=repository --overwrite
          echo "Configured Argo CD repository credentials in secret ${SECRET_NAME}"

      - name: Pre-install CloudNativePG CRDs (server-side apply)
        shell: bash
        run: |
          set -euo pipefail

          if ! command -v helm >/dev/null 2>&1; then
            echo "Helm not found on runner; installing Helm 3"
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          fi

          echo "Adding CloudNativePG Helm repository"
          helm repo add cloudnative-pg https://cloudnative-pg.github.io/charts --force-update >/dev/null 2>&1 || true

          echo "Updating CloudNativePG Helm repository index"
          for attempt in $(seq 1 5); do
            if helm repo update cloudnative-pg >/dev/null 2>&1; then
              echo "Helm repository index refreshed"
              break
            fi
            if [ "${attempt}" -eq 5 ]; then
              echo "Failed to update CloudNativePG Helm repository after ${attempt} attempts"
              exit 1
            fi
            echo "Helm repo update failed (attempt ${attempt}/5); retrying in 5 seconds"
            sleep 5
          done

          echo "Rendering CloudNativePG CRDs for version 0.22.1"
          if ! helm show crds cloudnative-pg/cloudnative-pg --version 0.22.1 > /tmp/cloudnative-pg-crds.yaml; then
            echo "helm show crds failed; aborting"
            exit 1
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "No CRDs were returned by 'helm show crds'; falling back to helm template rendering"
            helm template cloudnative-pg-crds cloudnative-pg/cloudnative-pg \
              --version 0.22.1 \
              --include-crds \
              --namespace cnpg-system > /tmp/cloudnative-pg-crds-rendered.yaml

            python3 <<'PY'
          import pathlib
          import sys

          rendered_path = pathlib.Path("/tmp/cloudnative-pg-crds-rendered.yaml")
          output_path = pathlib.Path("/tmp/cloudnative-pg-crds.yaml")

          if not rendered_path.exists():
              print("Rendered CRD manifest not found", file=sys.stderr)
              sys.exit(1)

          docs = []
          current = []
          for line in rendered_path.read_text().splitlines():
              if line.strip() == '---':
                  if current:
                      docs.append('\n'.join(current).strip())
                      current = []
                  continue
              current.append(line)
          if current:
              docs.append('\n'.join(current).strip())

          crd_docs = []
          for doc in docs:
              for line in doc.splitlines():
                  stripped = line.strip()
                  if stripped.startswith('kind:'):
                      if stripped.split(':', 1)[1].strip() == 'CustomResourceDefinition':
                          crd_docs.append(doc)
                      break

          if not crd_docs:
              print("No CustomResourceDefinition documents found in rendered template", file=sys.stderr)
              sys.exit(1)

          # Ensure each CRD document is separated by a YAML document marker on its
          # own line. Joining with "\n---\n" avoids concatenating the marker onto
          # the last line of the previous document when that document does not end
          # with a trailing newline (which would create invalid YAML like
          # "metadata:...---" and trigger kubectl parsing errors).
          output_path.write_text('\n---\n'.join(crd_docs) + '\n')

          PY
          fi

          if [ ! -s /tmp/cloudnative-pg-crds.yaml ]; then
            echo "Failed to render CloudNativePG CRDs"
            ls -l /tmp/cloudnative-pg-crds*
            exit 1
          fi

          echo "Applying CloudNativePG CRDs with server-side apply"
          kubectl apply --server-side -f /tmp/cloudnative-pg-crds.yaml

          echo "Removing potential last-applied annotations that exceed Kubernetes limits"
          for crd in \
            backups.postgresql.cnpg.io \
            clusterimagecatalogs.postgresql.cnpg.io \
            clusters.postgresql.cnpg.io \
            imagecatalogs.postgresql.cnpg.io \
            poolers.postgresql.cnpg.io \
            scheduledbackups.postgresql.cnpg.io; do
            if kubectl get crd "${crd}" >/dev/null 2>&1; then
              kubectl annotate crd "${crd}" kubectl.kubernetes.io/last-applied-configuration- >/dev/null 2>&1 || true
            fi
          done

      - name: Sync addons via Argo (Ingress-NGINX, cert-manager, CNPG operator)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/root-apps.yaml | kubectl apply -f -
          echo "Waiting for Argo CD application 'addons' to be created and synced"
          addons_ready=0
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application addons >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application addons -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "addons application is synced and healthy"
                addons_ready=1
                break
              fi
              echo "addons application status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "addons application not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          if [ "$addons_ready" -ne 1 ]; then
            echo "Timed out waiting for addons application to become healthy"
            kubectl -n argocd get application addons -o yaml || true
            exit 1
          fi
          kubectl -n argocd get application addons

      - name: Wait for cnpg-operator Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Argo CD application cnpg-operator to be created..."
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application cnpg-operator >/dev/null 2>&1; then
              sync_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
              health_status=$(kubectl -n argocd get application cnpg-operator -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
              if [ "$sync_status" = "Synced" ] && [ "$health_status" = "Healthy" ]; then
                echo "cnpg-operator application is synced and healthy"
                exit 0
              fi
              echo "cnpg-operator status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} (attempt ${attempt}/60)"
            else
              echo "Application cnpg-operator not found yet (attempt ${attempt}/60)"
            fi
            sleep 10
          done
          echo "Timed out waiting for cnpg-operator Argo CD application to become healthy"
          kubectl -n argocd get application cnpg-operator -o yaml || true
          exit 1

      - name: Wait for CNPG operator CRDs
        shell: bash
        run: |
          set -euo pipefail

          echo "Waiting for CloudNativePG CRDs to become available..."
          for attempt in $(seq 1 30); do
            if kubectl get crd clusters.postgresql.cnpg.io >/dev/null 2>&1; then
              if kubectl wait --for=condition=Established crd/clusters.postgresql.cnpg.io --timeout=60s; then
                if kubectl -n cnpg-system get deployment cnpg-cloudnative-pg >/dev/null 2>&1; then
                  if kubectl -n cnpg-system wait --for=condition=Available deployment/cnpg-cloudnative-pg --timeout=300s; then
                    echo "CloudNativePG operator deployment is available"
                    exit 0
                  else
                    echo "Deployment cnpg-cloudnative-pg exists but is not yet available (attempt ${attempt}/30)"
                  fi
                else
                  echo "Deployment cnpg-cloudnative-pg not found yet (attempt ${attempt}/30)"
                fi
              else
                echo "CRD clusters.postgresql.cnpg.io exists but is not yet established (attempt ${attempt}/30)"
              fi
            else
              echo "CRD clusters.postgresql.cnpg.io not found yet (attempt ${attempt}/30)"
            fi
            sleep 10
          done
          echo "Timed out waiting for CloudNativePG CRDs"
          exit 1

      - name: Wait for CNPG webhook service endpoints
        shell: bash
        run: |
          set -euo pipefail

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          echo "Ensuring cnpg-webhook-service has ready endpoints..."
          consecutive_ready=0
          for attempt in $(seq 1 45); do
            if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
              echo "cnpg-webhook-service not created yet (attempt ${attempt}/45)"
              consecutive_ready=0
            else
              collect_webhook_status
              log_webhook_status
              if [ -n "${WEBHOOK_READY_FROM_ENDPOINTS}" ] || [ -n "${WEBHOOK_READY_FROM_SLICES}" ]; then
                consecutive_ready=$((consecutive_ready + 1))
                echo "Ready endpoints observed (${consecutive_ready}/3 consecutive confirmations)"
                if [ "${consecutive_ready}" -ge 3 ]; then
                  echo "cnpg-webhook-service has ready endpoints that appear stable"
                  echo "Waiting a few seconds to let the Kubernetes API server register the webhook endpoints"
                  sleep 10
                  exit 0
                fi
                sleep 5
                continue
              fi

              echo "cnpg-webhook-service endpoints not ready yet (attempt ${attempt}/45)"
              consecutive_ready=0
            fi
            sleep 10
          done

          echo "Timed out waiting for cnpg-webhook-service endpoints"
          kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
          kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
          kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
          exit 1

      - name: Create CNPG secrets (DB users + superuser)
        shell: bash
        run: |
          set -euo pipefail

          ns="${{ inputs.NAMESPACE_IAM }}"

          if [ -z "${ns}" ]; then
            echo "NAMESPACE_IAM input must not be empty"
            exit 1
          fi

          ensure_secret_type() {
            local secret_name="$1"
            local expected_type="$2"

            if kubectl -n "${ns}" get secret "${secret_name}" >/dev/null 2>&1; then
              local current_type
              current_type=$(kubectl -n "${ns}" get secret "${secret_name}" -o jsonpath='{.type}')

              if [ "${current_type}" != "${expected_type}" ]; then
                echo "Secret ${secret_name} exists with type ${current_type}; recreating with type ${expected_type}"
                kubectl -n "${ns}" delete secret "${secret_name}" --wait=false --ignore-not-found
                echo "Waiting for secret ${secret_name} to be fully removed before recreating"
                for attempt in $(seq 1 12); do
                  if ! kubectl -n "${ns}" get secret "${secret_name}" >/dev/null 2>&1; then
                    echo "Secret ${secret_name} removed"
                    break
                  fi

                  if [ "${attempt}" -eq 12 ]; then
                    echo "Secret ${secret_name} still present after waiting; aborting"
                    exit 1
                  fi

                  sleep 5
                done
              fi
            fi
          }

          apply_basic_auth_secret() {
            local secret_name="$1"
            local username="$2"
            local password="$3"
            local password_source="$4"

            if [ -z "${password}" ]; then
              echo "ERROR: password value for secret ${secret_name} is empty (expected GitHub secret ${password_source})"
              exit 1
            fi

            ensure_secret_type "${secret_name}" "kubernetes.io/basic-auth"

            kubectl -n "${ns}" create secret generic "${secret_name}" \
              --type=kubernetes.io/basic-auth \
              --from-literal=username="${username}" \
              --from-literal=password="${password}" \
              --dry-run=client -o yaml | kubectl apply -f -

            echo "Secret ${secret_name} ensured with type kubernetes.io/basic-auth"
          }

          apply_basic_auth_secret "cnpg-superuser" "postgres" "${{ secrets.POSTGRES_SUPERUSER_PASSWORD }}" "POSTGRES_SUPERUSER_PASSWORD"
          apply_basic_auth_secret "keycloak-db-app" "keycloak" "${{ secrets.KEYCLOAK_DB_PASSWORD }}" "KEYCLOAK_DB_PASSWORD"
          apply_basic_auth_secret "midpoint-db-app" "midpoint" "${{ secrets.MIDPOINT_DB_PASSWORD }}" "MIDPOINT_DB_PASSWORD"

      - name: Create Azure Blob secret for CNPG backups (connection string or key)
        shell: bash
        env:
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail

          if [ -z "$AZURE_STORAGE_KEY" ]; then
            echo "AZURE_STORAGE_KEY secret is required for demo backup. Add it in repo secrets."
            exit 1
          fi
          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic cnpg-azure-backup \
            --from-literal=AZURE_STORAGE_ACCOUNT="$AZURE_STORAGE_ACCOUNT" \
            --from-literal=AZURE_STORAGE_KEY="$AZURE_STORAGE_KEY" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Purge existing CNPG Azure backup prefix
        shell: bash
        env:
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail

          container="cnpg-backups"
          prefix="iam-db"

          if [ -z "${AZURE_STORAGE_ACCOUNT}" ]; then
            echo "STORAGE_ACCOUNT input must not be empty"
            exit 1
          fi

          if [ -z "${AZURE_STORAGE_KEY}" ]; then
            echo "AZURE_STORAGE_KEY secret missing; skipping Azure backup purge"
            exit 0
          fi

          echo "Ensuring Azure Blob container ${container} exists in account ${AZURE_STORAGE_ACCOUNT}"

          use_connection_string=false
          use_sas_token=false
          if [[ "${AZURE_STORAGE_KEY}" == *"DefaultEndpointsProtocol="* ]] || \
             [[ "${AZURE_STORAGE_KEY}" == *"AccountKey="* ]] || \
             [[ "${AZURE_STORAGE_KEY}" == *"BlobEndpoint="* ]]; then
            use_connection_string=true
            echo "AZURE_STORAGE_KEY appears to be an Azure Storage connection string; using it for az storage commands"
          elif [[ "${AZURE_STORAGE_KEY}" == \?sv=* ]] || \
               [[ "${AZURE_STORAGE_KEY}" == sv=* ]] || \
               ([[ "${AZURE_STORAGE_KEY}" == *"sig="* ]] && [[ "${AZURE_STORAGE_KEY}" == *"se="* ]]); then
            use_sas_token=true
            echo "AZURE_STORAGE_KEY appears to be an Azure Storage SAS token; using it for az storage commands"
          fi

          az_auth_args=()
          if [ "${use_connection_string}" = true ]; then
            az_auth_args+=("--connection-string" "${AZURE_STORAGE_KEY}")
          elif [ "${use_sas_token}" = true ]; then
            sas_token="${AZURE_STORAGE_KEY#\?}"
            az_auth_args+=("--account-name" "${AZURE_STORAGE_ACCOUNT}" "--sas-token" "${sas_token}")
          else
            az_auth_args+=("--account-name" "${AZURE_STORAGE_ACCOUNT}")
            if [ "${use_sas_token}" = true ]; then
              sas_token="${AZURE_STORAGE_KEY}"
              if [[ "${sas_token}" == https://* ]]; then
                sas_token="${sas_token#*\?}"
              fi
              if [[ "${sas_token}" != \?* ]]; then
                sas_token="?${sas_token}"
              fi
              az_auth_args+=("--sas-token" "${sas_token}")
            else
              az_auth_args+=("--account-key" "${AZURE_STORAGE_KEY}")
            fi
          fi

          container_exists=$(az storage container exists \
            --name "${container}" \
            "${az_auth_args[@]}" \
            --query exists -o tsv 2>/dev/null || true)

          if [ "${container_exists}" != "true" ]; then
            echo "Azure container ${container} not found; creating it"
            az storage container create \
              --name "${container}" \
              "${az_auth_args[@]}" \
              --public-access off 1>/dev/null
          fi

          echo "Checking for leftover WAL/archive objects under ${container}/${prefix}"
          existing_count=$(az storage blob list \
            "${az_auth_args[@]}" \
            --container-name "${container}" \
            --prefix "${prefix}/" \
            --query "length(@)" -o tsv 2>/dev/null || echo 0)

          if [ "${existing_count}" = "" ]; then
            existing_count=0
          fi

          if [ "${existing_count}" -eq 0 ]; then
            echo "Azure backup prefix ${prefix}/ is already empty"
            exit 0
          fi

          echo "Deleting ${existing_count} Azure Blob object(s) under ${prefix}/ before bootstrapping CNPG"
          az storage blob delete-batch \
            "${az_auth_args[@]}" \
            --source "${container}" \
            --pattern "${prefix}/*" \
            --no-progress

          echo "Verifying Azure backup prefix ${prefix}/ is now empty"
          remaining_count=$(az storage blob list \
            "${az_auth_args[@]}" \
            --container-name "${container}" \
            --prefix "${prefix}/" \
            --query "length(@)" -o tsv 2>/dev/null || echo 0)

          if [ "${remaining_count}" = "" ]; then
            remaining_count=0
          fi

          if [ "${remaining_count}" -ne 0 ]; then
            echo "Failed to purge Azure backup prefix ${prefix}/ (still ${remaining_count} object(s) present)"
            exit 1
          fi

          echo "Azure backup prefix ${prefix}/ successfully emptied"

      - name: Validate CNPG prerequisites
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          ns="${NAMESPACE_IAM}"
          echo "Checking required database secrets exist in namespace ${ns}"
          missing=0
          for secret in cnpg-superuser keycloak-db-app midpoint-db-app cnpg-azure-backup; do
            if ! kubectl -n "${ns}" get secret "${secret}" >/dev/null 2>&1; then
              echo "ERROR: secret ${secret} is missing from namespace ${ns}"
              missing=1
            fi
          done

          if [ "${missing}" -ne 0 ]; then
            echo "One or more required secrets are missing; aborting."
            exit 1
          fi

          check_secret_key() {
            local secret="$1"
            local key="$2"
            local value
            value=$(kubectl -n "${ns}" get secret "${secret}" -o jsonpath="{.data.${key}}" 2>/dev/null || true)
            if [ -z "${value}" ]; then
              echo "ERROR: secret ${secret} does not contain key ${key}"
              missing=1
            fi
          }

          check_secret_key cnpg-superuser username
          check_secret_key cnpg-superuser password
          check_secret_key keycloak-db-app username
          check_secret_key keycloak-db-app password
          check_secret_key midpoint-db-app username
          check_secret_key midpoint-db-app password
          check_secret_key cnpg-azure-backup AZURE_STORAGE_ACCOUNT
          check_secret_key cnpg-azure-backup AZURE_STORAGE_KEY

          if [ "${missing}" -ne 0 ]; then
            echo "Secret validation failed; aborting."
            exit 1
          fi

          echo "Confirming CNPG operator deployment readiness"
          kubectl -n cnpg-system get deployment cnpg-cloudnative-pg
          if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
            echo "WARNING: cnpg-cloudnative-pg deployment not yet available"
          fi

          echo "Inspecting CNPG webhook service endpoints"
          if ! kubectl -n cnpg-system get service cnpg-webhook-service >/dev/null 2>&1; then
            echo "ERROR: cnpg-webhook-service not found in cnpg-system namespace"
            exit 1
          fi

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          collect_webhook_status
          log_webhook_status
          if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
            echo "ERROR: cnpg-webhook-service currently has no ready endpoints"
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            exit 1
          fi

          ready_endpoints="${WEBHOOK_READY_FROM_ENDPOINTS:-${WEBHOOK_READY_FROM_SLICES}}"

          echo "cnpg-webhook-service ready endpoints: ${ready_endpoints:-<unknown>}"

      - name: Apply CNPG cluster (iam-db)
        env:
          STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          manifest="$(mktemp)"
          trap 'rm -f "${manifest}"' EXIT
          sed "s/{{STORAGE_ACCOUNT}}/${STORAGE_ACCOUNT}/g" k8s/apps/cnpg/cluster.yaml > "${manifest}"

          WEBHOOK_READY_FROM_ENDPOINTS=""
          WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
          WEBHOOK_READY_FROM_SLICES=""
          WEBHOOK_NOT_READY_FROM_SLICES=""

          collect_webhook_status() {
            WEBHOOK_READY_FROM_ENDPOINTS=""
            WEBHOOK_NOT_READY_FROM_ENDPOINTS=""
            WEBHOOK_READY_FROM_SLICES=""
            WEBHOOK_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n cnpg-system get endpoints cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o json 2>/dev/null); then
              WEBHOOK_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              WEBHOOK_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_webhook_status() {
            echo "cnpg-webhook-service ready IPs (Endpoints): ${WEBHOOK_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service notReady IPs (Endpoints): ${WEBHOOK_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "cnpg-webhook-service ready IPs (EndpointSlices): ${WEBHOOK_READY_FROM_SLICES:-<none>}"
            echo "cnpg-webhook-service notReady IPs (EndpointSlices): ${WEBHOOK_NOT_READY_FROM_SLICES:-<none>}"
          }

          is_webhook_unavailable_error() {
            local message="$1"
            if grep -qi 'no endpoints available for service "cnpg-webhook-service"' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook "mcluster.cnpg.io"' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            if grep -qi 'failed calling webhook' <<<"${message}" && grep -qi 'cnpg-webhook-service' <<<"${message}"; then
              return 0
            fi
            return 1
          }

          max_attempts=8
          success=0
          for attempt in $(seq 1 "${max_attempts}"); do
            echo "Applying CNPG cluster manifest (attempt ${attempt}/${max_attempts})"

            prereqs_ready=1
            if ! kubectl -n cnpg-system rollout status deployment/cnpg-cloudnative-pg --timeout=180s; then
              echo "cnpg-cloudnative-pg deployment not yet available"
              prereqs_ready=0
            fi

            if ! kubectl -n cnpg-system wait --for=condition=Ready pod -l app.kubernetes.io/name=cloudnative-pg --timeout=180s; then
              echo "CNPG operator pods are not yet in Ready state"
              prereqs_ready=0
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service has no ready endpoints before attempt ${attempt}"
              prereqs_ready=0
            fi

            if [ "${prereqs_ready}" -ne 1 ]; then
              echo "CNPG operator prerequisites are not ready, waiting before retrying"
              sleep 20
              continue
            fi

            echo "Running server-side dry run to verify CNPG webhooks are reachable"
            if dry_run_output=$(kubectl apply --dry-run=server -f "${manifest}" 2>&1); then
              echo "${dry_run_output}"
            else
              echo "${dry_run_output}"
              if is_webhook_unavailable_error "${dry_run_output}"; then
                echo "Server-side dry run indicates CNPG webhooks are unavailable; retrying after delay"
                sleep 20
                continue
              fi
              echo "Server-side dry run failed for an unexpected reason; showing diagnostics"
              kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
              kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
              kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
              kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
              sleep 20
              continue
            fi

            collect_webhook_status
            log_webhook_status
            if [ -z "${WEBHOOK_READY_FROM_ENDPOINTS}" ] && [ -z "${WEBHOOK_READY_FROM_SLICES}" ]; then
              echo "cnpg-webhook-service endpoints disappeared after dry run; waiting before retry"
              sleep 20
              continue
            fi

            if apply_output=$(kubectl apply -f "${manifest}" 2>&1); then
              echo "${apply_output}"
              echo "CNPG cluster manifest applied successfully"
              success=1
              break
            else
              echo "${apply_output}"
              if is_webhook_unavailable_error "${apply_output}"; then
                echo "kubectl apply failed because CNPG webhooks were unavailable; retrying after gathering diagnostics"
              else
                echo "kubectl apply failed on attempt ${attempt}; showing CNPG operator diagnostics"
              fi
            fi

            kubectl -n cnpg-system get pods -l app.kubernetes.io/name=cloudnative-pg -o wide || true
            kubectl -n cnpg-system describe deployment cnpg-cloudnative-pg || true
            kubectl -n cnpg-system get endpoints cnpg-webhook-service -o yaml || true
            kubectl -n cnpg-system get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=cnpg-webhook-service -o yaml || true
            sleep 20
          done

          if [ "${success:-0}" -ne 1 ]; then
            echo "Failed to apply CNPG cluster manifest after ${max_attempts} attempts"
            exit 1
          fi

      - name: Ensure midPoint database and role exist
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        shell: bash
        run: |
          set -euo pipefail

          ns="${NAMESPACE_IAM}"
          if [ -z "${ns}" ]; then
            echo "NAMESPACE_IAM input must not be empty"
            exit 1
          fi

          echo "Waiting for CloudNativePG cluster iam-db resource to be created"
          cluster_found=0
          for attempt in $(seq 1 30); do
            if kubectl -n "${ns}" get cluster iam-db >/dev/null 2>&1; then
              cluster_found=1
              break
            fi
            echo "Cluster iam-db not found yet in namespace ${ns} (attempt ${attempt}/30)"
            sleep 10
          done

          if [ "${cluster_found}" -ne 1 ]; then
            echo "Timed out waiting for Cluster iam-db resource to be created"
            kubectl -n "${ns}" get cluster iam-db -o yaml || true
            exit 1
          fi

          echo "Waiting for CloudNativePG cluster iam-db Ready condition"
          if ! kubectl -n "${ns}" wait --for=condition=Ready cluster/iam-db --timeout=600s; then
            echo "Cluster iam-db did not reach Ready condition within timeout"
            kubectl -n "${ns}" get cluster iam-db -o yaml || true
            kubectl -n "${ns}" get pods -l cnpg.io/cluster=iam-db -o wide || true
            kubectl -n "${ns}" describe cluster iam-db || true
            exit 1
          fi

          DB_READY_FROM_ENDPOINTS=""
          DB_NOT_READY_FROM_ENDPOINTS=""
          DB_READY_FROM_SLICES=""
          DB_NOT_READY_FROM_SLICES=""

          collect_db_endpoint_status() {
            DB_READY_FROM_ENDPOINTS=""
            DB_NOT_READY_FROM_ENDPOINTS=""
            DB_READY_FROM_SLICES=""
            DB_NOT_READY_FROM_SLICES=""

            if endpoints_json=$(kubectl -n "${ns}" get endpoints iam-db-rw -o json 2>/dev/null); then
              DB_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .addresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
              DB_NOT_READY_FROM_ENDPOINTS=$(jq -r '[.subsets[]? | .notReadyAddresses[]? | .ip] | join(" ")' <<<"${endpoints_json}" 2>/dev/null || true)
            fi

            if endpointslices_json=$(kubectl -n "${ns}" get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=iam-db-rw -o json 2>/dev/null); then
              DB_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready == true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
              DB_NOT_READY_FROM_SLICES=$(jq -r '[.items[]? | .endpoints[]? | select(.conditions.ready != true) | .addresses[]?] | join(" ")' <<<"${endpointslices_json}" 2>/dev/null || true)
            fi
          }

          log_db_endpoint_status() {
            echo "iam-db-rw ready IPs (Endpoints): ${DB_READY_FROM_ENDPOINTS:-<none>}"
            echo "iam-db-rw notReady IPs (Endpoints): ${DB_NOT_READY_FROM_ENDPOINTS:-<none>}"
            echo "iam-db-rw ready IPs (EndpointSlices): ${DB_READY_FROM_SLICES:-<none>}"
            echo "iam-db-rw notReady IPs (EndpointSlices): ${DB_NOT_READY_FROM_SLICES:-<none>}"
          }

          echo "Waiting for iam-db read/write service endpoints in namespace ${ns}"
          endpoints=""
          consecutive_ready=0
          max_attempts=45
          for attempt in $(seq 1 "${max_attempts}"); do
            collect_db_endpoint_status
            log_db_endpoint_status

            if [ -n "${DB_READY_FROM_ENDPOINTS}" ] || [ -n "${DB_READY_FROM_SLICES}" ]; then
              endpoints="${DB_READY_FROM_ENDPOINTS:-${DB_READY_FROM_SLICES}}"
              consecutive_ready=$((consecutive_ready + 1))
              echo "iam-db-rw has ready endpoints (${consecutive_ready}/3 consecutive confirmations)"
              if [ "${consecutive_ready}" -ge 3 ]; then
                break
              fi
              sleep 5
              continue
            fi

            echo "iam-db-rw service has no ready endpoints yet (attempt ${attempt}/${max_attempts})"
            consecutive_ready=0
            sleep 10
          done

          if [ -z "${endpoints}" ]; then
            echo "Timed out waiting for iam-db-rw service endpoints"
            kubectl -n "${ns}" get svc iam-db-rw -o yaml || true
            kubectl -n "${ns}" get endpoints iam-db-rw -o yaml || true
            kubectl -n "${ns}" get endpointslices.discovery.k8s.io -l kubernetes.io/service-name=iam-db-rw -o yaml || true
            kubectl -n "${ns}" get pods -l cnpg.io/cluster=iam-db -o wide || true
            kubectl -n "${ns}" describe cluster iam-db || true
            exit 1
          fi

          echo "iam-db-rw endpoints: ${endpoints}"

          job_name="midpoint-db-bootstrap"
          kubectl -n "${ns}" delete job "${job_name}" --ignore-not-found

          manifest="$(mktemp)"
          cleanup_manifest() {
            rm -f "${manifest}"
          }
          trap cleanup_manifest EXIT

          cat <<'YAML' | sed 's/^          //' >"${manifest}"
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: midpoint-db-bootstrap
            namespace: ${NS}
          spec:
            backoffLimit: 3
            template:
              spec:
                restartPolicy: Never
                containers:
                  - name: psql
                    image: ghcr.io/cloudnative-pg/postgresql:16.4
                    env:
                      - name: PGPASSWORD
                        valueFrom:
                          secretKeyRef:
                            name: cnpg-superuser
                            key: password
                      - name: MIDPOINT_DB_USER
                        valueFrom:
                          secretKeyRef:
                            name: midpoint-db-app
                            key: username
                      - name: MIDPOINT_DB_PASSWORD
                        valueFrom:
                          secretKeyRef:
                            name: midpoint-db-app
                            key: password
                      - name: DB_HOST
                        value: iam-db-rw.${NS}.svc.cluster.local
                    command:
                      - bash
                      - -lc
                      - |
                        set -euo pipefail
                        psql -h "${DB_HOST}" -U postgres -v ON_ERROR_STOP=1 \
                          --set=mp_user="${MIDPOINT_DB_USER}" \
                          --set=mp_password="${MIDPOINT_DB_PASSWORD}" <<'SQL'
                        DO $do$
                        DECLARE
                          role_name text := :'mp_user';
                          role_password text := :'mp_password';
                        BEGIN
                          IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = role_name) THEN
                            EXECUTE format('CREATE ROLE %I LOGIN PASSWORD %L', role_name, role_password);
                          ELSE
                            EXECUTE format('ALTER ROLE %I PASSWORD %L', role_name, role_password);
                            EXECUTE format('ALTER ROLE %I LOGIN', role_name);
                          END IF;
                        END
                          $do$;

                          DO $do$
                          DECLARE
                            role_name text := :'mp_user';
                          BEGIN
                            IF NOT EXISTS (SELECT 1 FROM pg_database WHERE datname = 'midpoint') THEN
                              EXECUTE format('CREATE DATABASE %I OWNER %I', 'midpoint', role_name);
                            ELSE
                              EXECUTE format('ALTER DATABASE %I OWNER TO %I', 'midpoint', role_name);
                            END IF;
                          END
                          $do$;
                          SQL
          YAML
          export NS="${ns}"
          envsubst '$NS' < "${manifest}" | kubectl apply -f -

          if ! kubectl -n "${ns}" wait --for=condition=Complete job/"${job_name}" --timeout=240s; then
            echo "midPoint database bootstrap job did not complete successfully"
            kubectl -n "${ns}" logs job/"${job_name}" || true
            kubectl -n "${ns}" describe job "${job_name}" || true
            kubectl -n "${ns}" get pods -l job-name="${job_name}" -o wide || true
            exit 1
          fi

          kubectl -n "${ns}" logs job/"${job_name}" || true
          kubectl -n "${ns}" delete job "${job_name}" --ignore-not-found

          kubectl -n "${ns}" wait cluster/iam-db --for=condition=Ready --timeout=600s || true

      - name: Install Keycloak Operator (CRDs + operator Deployment)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloaks.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml
          kubectl apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/26.3.4/kubernetes/kubernetes.yml

      - name: Prepare midPoint config and admin secret
        shell: bash
        env:
          MIDPOINT_ADMIN_PASSWORD: ${{ secrets.MIDPOINT_ADMIN_PASSWORD }}
        run: |
          set -euo pipefail

          kubectl -n ${{ inputs.NAMESPACE_IAM }} create secret generic midpoint-admin \
            --from-literal=password="$MIDPOINT_ADMIN_PASSWORD" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n ${{ inputs.NAMESPACE_IAM }} create configmap midpoint-config \
            --from-file=config.xml=k8s/apps/midpoint/config.xml \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Wait for Keycloak operator CRDs
        shell: bash
        run: |
          set -euo pipefail
          echo "Waiting for Keycloak CRDs to become available..."
          for crd in keycloaks.k8s.keycloak.org keycloakrealmimports.k8s.keycloak.org; do
            kubectl wait --for=condition=Established crd/${crd} --timeout=300s
          done
          for ns in keycloak-system keycloak default; do
            if kubectl -n "$ns" get deployment keycloak-operator >/dev/null 2>&1; then
              kubectl -n "$ns" wait --for=condition=Available deployment/keycloak-operator --timeout=300s || true
              break
            fi
          done

      - name: Create Argo CD application for iam apps (Keycloak + midPoint)
        shell: bash
        run: |
          set -euo pipefail
          export REPO_OWNER="${GITHUB_REPOSITORY%%/*}"
          export REPO_NAME="${GITHUB_REPOSITORY##*/}"
          envsubst < k8s/argocd/apps.yaml | kubectl apply -f -

      - name: Wait for iam apps Argo CD application
        shell: bash
        run: |
          set -euo pipefail
          echo "Ensuring Argo CD application 'apps' has been created before monitoring status"
          app_found=0
          for attempt in $(seq 1 60); do
            if kubectl -n argocd get application apps >/dev/null 2>&1; then
              app_found=1
              echo "Argo CD application 'apps' detected"
              break
            fi
            echo "Application 'apps' not found yet (attempt ${attempt}/60)"
            sleep 10
          done

          if [ "${app_found}" -ne 1 ]; then
            echo "Timed out waiting for Argo CD application 'apps' to be created"
            kubectl -n argocd get applications || true
            exit 1
          fi

          echo "Waiting for Argo CD application 'apps' to report Synced/Healthy status"
          diagnostics_dumped=0
          for attempt in $(seq 1 90); do
            sync_status=$(kubectl -n argocd get application apps -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "")
            health_status=$(kubectl -n argocd get application apps -o jsonpath='{.status.health.status}' 2>/dev/null || echo "")
            operation_phase=$(kubectl -n argocd get application apps -o jsonpath='{.status.operationState.phase}' 2>/dev/null || echo "")

            echo "apps status: sync=${sync_status:-<unknown>} health=${health_status:-<unknown>} operation=${operation_phase:-<unknown>} (attempt ${attempt}/90)"

            if [ "${sync_status}" = "Synced" ]; then
              if [ "${health_status}" = "Healthy" ]; then
                echo "apps application is synced and healthy"
                kubectl -n argocd get application apps
                exit 0
              fi
              if [ "${health_status}" = "Unknown" ] && [ "${operation_phase}" = "Succeeded" ]; then
                echo "apps application is synced; health reported as Unknown but last operation succeeded. Proceeding."
                kubectl -n argocd get application apps
                exit 0
              fi
            fi

            if [ "${operation_phase}" = "Failed" ] && [ "${diagnostics_dumped}" -eq 0 ]; then
              echo "Latest Argo CD sync operation failed. Dumping application manifest for troubleshooting."
              kubectl -n argocd get application apps -o yaml || true
              diagnostics_dumped=1
            fi

            if [ "${health_status}" = "Degraded" ] && [ "${diagnostics_dumped}" -eq 0 ]; then
              echo "apps application health is Degraded. Dumping application manifest for troubleshooting."
              kubectl -n argocd get application apps -o yaml || true
              diagnostics_dumped=1
            fi

            sleep 10
          done

          echo "Timed out waiting for Argo CD application 'apps' to become synced and healthy"
          kubectl -n argocd get application apps -o yaml || true
          exit 1

      - name: Show ingress endpoints (if available)
        shell: bash
        run: |
          set -euo pipefail
          echo "Ingress-NGINX service:"
          kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide || true
          echo "Keycloak service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc rws-keycloak -o wide || true
          echo "midPoint service:"
          kubectl -n ${{ inputs.NAMESPACE_IAM }} get svc midpoint -o wide || true
