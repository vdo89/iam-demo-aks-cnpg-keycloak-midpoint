name: 02 - Bootstrap GitOps stack

on:
  workflow_dispatch:
    inputs:
      RESOURCE_GROUP:
        description: 'Resource Group name (Terraform output)'
        required: true
        default: 'rwsdemo-rg'
      AKS_NAME:
        description: 'AKS cluster name (Terraform output)'
        required: true
        default: 'rwsdemo-aks'
      STORAGE_ACCOUNT:
        description: 'Azure Storage Account name for CNPG backups (Terraform output)'
        required: true
      NAMESPACE_IAM:
        description: 'Namespace for IAM workloads (Keycloak + midPoint + CNPG)'
        required: false
        default: 'iam'

permissions:
  id-token: write
  contents: write

jobs:
  bootstrap:
    runs-on: ubuntu-latest
    env:
      ARGOCD_REPO_USERNAME: ${{ secrets.ARGOCD_REPO_USERNAME }}
      ARGOCD_REPO_TOKEN: ${{ secrets.ARGOCD_REPO_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - name: Install test dependencies
        shell: bash
        run: |
          set -euo pipefail
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements-dev.txt

      - name: Run GitOps validation tests
        shell: bash
        run: |
          set -euo pipefail
          pytest -q

      - name: Azure Login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AKS credentials
        uses: azure/aks-set-context@v4
        with:
          resource-group: ${{ inputs.RESOURCE_GROUP }}
          cluster-name: ${{ inputs.AKS_NAME }}

      - name: Install Argo CD (kustomize bootstrap)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -k gitops/clusters/aks/bootstrap
          echo "Waiting for Argo CD control plane to become ready"
          for workload in \
            deploy/argocd-server \
            deploy/argocd-repo-server \
            deploy/argocd-redis \
            deploy/argocd-dex-server; do
            echo "Waiting for rollout of ${workload}"
            kubectl -n argocd rollout status "${workload}" --timeout=300s
          done
          kubectl -n argocd rollout status statefulset/argocd-application-controller --timeout=300s

      - name: Configure Argo CD repository credentials (private repos only)
        if: ${{ env.ARGOCD_REPO_USERNAME != '' && env.ARGOCD_REPO_TOKEN != '' }}
        shell: bash
        env:
          GITHUB_REPOSITORY: ${{ github.repository }}
          ARGOCD_REPO_USERNAME: ${{ env.ARGOCD_REPO_USERNAME }}
          ARGOCD_REPO_TOKEN: ${{ env.ARGOCD_REPO_TOKEN }}
        run: |
          set -euo pipefail
          owner="${GITHUB_REPOSITORY%%/*}"
          repo="${GITHUB_REPOSITORY##*/}"
          sanitized=$(printf '%s' "${owner}-${repo}" | tr '[:upper:]' '[:lower:]' | tr -c 'a-z0-9' '-' | sed 's/^-*//;s/-*$//')
          secret_name="repo-${sanitized:-repo}"
          kubectl -n argocd create secret generic "${secret_name}" \
            --type=Opaque \
            --from-literal=url="https://github.com/${owner}/${repo}" \
            --from-literal=username="${ARGOCD_REPO_USERNAME}" \
            --from-literal=password="${ARGOCD_REPO_TOKEN}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n argocd label secret "${secret_name}" argocd.argoproj.io/secret-type=repository --overwrite

      - name: Render CNPG storage account configuration
        shell: bash
        env:
          STORAGE_ACCOUNT_INPUT: ${{ inputs.STORAGE_ACCOUNT }}
        run: |
          set -euo pipefail
          config_file="gitops/apps/iam/cnpg/params.env"
          storage_account_input=$(printf '%s' "${STORAGE_ACCOUNT_INPUT}" | tr -d ' \t\r\n')
          if [ -z "${storage_account_input}" ]; then
            echo "STORAGE_ACCOUNT input must not be empty."
            exit 1
          fi
          tmp_file=$(mktemp)
          cat >"${tmp_file}" <<EOF
          # storageAccount is set by the bootstrap workflow from the STORAGE_ACCOUNT input.
          storageAccount=${storage_account_input}
          EOF
          mv "${tmp_file}" "${config_file}"

      - name: Render GitOps context configuration
        shell: bash
        env:
          GITOPS_REPO_URL: ${{ github.server_url }}/${{ github.repository }}
          GITOPS_TARGET_REVISION: ${{ github.ref_name }}
        run: |
          set -euo pipefail
          cat <<EOF > gitops/clusters/aks/context.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: gitops-context
            namespace: argocd
          data:
            repoURL: ${GITOPS_REPO_URL}
            targetRevision: ${GITOPS_TARGET_REVISION}
          EOF

      - name: Create IAM namespace early
        shell: bash
        run: |
          set -euo pipefail
          kubectl create namespace "${{ inputs.NAMESPACE_IAM }}" --dry-run=client -o yaml | kubectl apply -f -

      - name: Reset CNPG and Keycloak workloads
        shell: bash
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        run: |
          set -euo pipefail

          echo "Ensuring previous CloudNativePG and Keycloak resources are removed before bootstrap."

          if kubectl get crd clusters.postgresql.cnpg.io >/dev/null 2>&1; then
            if kubectl -n "${NAMESPACE_IAM}" get cluster iam-db >/dev/null 2>&1; then
              echo "Deleting existing CloudNativePG cluster iam-db in namespace ${NAMESPACE_IAM}."
              kubectl -n "${NAMESPACE_IAM}" delete cluster iam-db --wait=true --ignore-not-found
              echo "CloudNativePG cluster iam-db deleted."
            else
              echo "No existing CloudNativePG cluster iam-db found in namespace ${NAMESPACE_IAM}."
            fi
          else
            echo "CloudNativePG Cluster CRD not present; skipping CNPG cleanup."
          fi

          if kubectl get crd keycloaks.k8s.keycloak.org >/dev/null 2>&1; then
            if kubectl -n "${NAMESPACE_IAM}" get keycloak rws-keycloak >/dev/null 2>&1; then
              echo "Deleting existing Keycloak resource rws-keycloak in namespace ${NAMESPACE_IAM}."
              kubectl -n "${NAMESPACE_IAM}" delete keycloak rws-keycloak --wait=true --ignore-not-found
              echo "Keycloak resource rws-keycloak deleted."
            else
              echo "No existing Keycloak resource rws-keycloak found in namespace ${NAMESPACE_IAM}."
            fi
          else
            echo "Keycloak CRD not present; skipping Keycloak cleanup."
          fi

      - name: Seed database credentials
        shell: bash
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
          POSTGRES_SUPERUSER_PASSWORD: ${{ secrets.POSTGRES_SUPERUSER_PASSWORD }}
          KEYCLOAK_DB_PASSWORD: ${{ secrets.KEYCLOAK_DB_PASSWORD }}
          MIDPOINT_DB_PASSWORD: ${{ secrets.MIDPOINT_DB_PASSWORD }}
        run: |
          set -euo pipefail
          if [ -z "${POSTGRES_SUPERUSER_PASSWORD}" ] || [ -z "${KEYCLOAK_DB_PASSWORD}" ] || [ -z "${MIDPOINT_DB_PASSWORD}" ]; then
            echo "Database credentials are required (POSTGRES_SUPERUSER_PASSWORD, KEYCLOAK_DB_PASSWORD, MIDPOINT_DB_PASSWORD)."
            exit 1
          fi
          for secret in cnpg-superuser keycloak-db-app midpoint-db-app; do
            kubectl -n "${NAMESPACE_IAM}" delete secret "${secret}" --ignore-not-found >/dev/null 2>&1 || true
          done
          kubectl -n "${NAMESPACE_IAM}" create secret generic cnpg-superuser \
            --type=Opaque \
            --from-literal=username=postgres \
            --from-literal=password="${POSTGRES_SUPERUSER_PASSWORD}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n "${NAMESPACE_IAM}" create secret generic keycloak-db-app \
            --type=Opaque \
            --from-literal=username=keycloak \
            --from-literal=password="${KEYCLOAK_DB_PASSWORD}" \
            --dry-run=client -o yaml | kubectl apply -f -
          kubectl -n "${NAMESPACE_IAM}" create secret generic midpoint-db-app \
            --type=Opaque \
            --from-literal=username=midpoint \
            --from-literal=password="${MIDPOINT_DB_PASSWORD}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Create midPoint administrator secret
        shell: bash
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
          MIDPOINT_ADMIN_PASSWORD: ${{ secrets.MIDPOINT_ADMIN_PASSWORD }}
        run: |
          set -euo pipefail
          if [ -z "${MIDPOINT_ADMIN_PASSWORD}" ]; then
            echo "MIDPOINT_ADMIN_PASSWORD secret is required."
            exit 1
          fi
          kubectl -n "${NAMESPACE_IAM}" delete secret midpoint-admin --ignore-not-found >/dev/null 2>&1 || true
          kubectl -n "${NAMESPACE_IAM}" create secret generic midpoint-admin \
            --type=Opaque \
            --from-literal=username=administrator \
            --from-literal=password="${MIDPOINT_ADMIN_PASSWORD}" \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Normalize Azure Blob credentials for CNPG backups
        shell: bash
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
          AZURE_STORAGE_ACCOUNT: ${{ inputs.STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail
          if [ -z "${AZURE_STORAGE_KEY}" ]; then
            echo "AZURE_STORAGE_KEY secret must provide a storage account key, SAS token, or connection string."
            exit 1
          fi
          python3 scripts/normalize_azure_storage_secret.py \
            --namespace "${NAMESPACE_IAM}" \
            --storage-account "${AZURE_STORAGE_ACCOUNT}" \
            --credential "${AZURE_STORAGE_KEY}"

      - name: Apply GitOps applications
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -k gitops/clusters/aks

      - name: Wait for Keycloak operator CRDs
        shell: bash
        run: |
          set -euo pipefail
          required_crds=(keycloaks.k8s.keycloak.org keycloakrealmimports.k8s.keycloak.org)
          missing_crds=("${required_crds[@]}")
          for attempt in $(seq 1 12); do
            missing_crds=()
            for crd in "${required_crds[@]}"; do
              if ! kubectl get crd "${crd}" >/dev/null 2>&1; then
                missing_crds+=("${crd}")
              fi
            done

            if [ "${#missing_crds[@]}" -eq 0 ]; then
              echo "All Keycloak operator CRDs are present."
              exit 0
            fi

            echo "Attempt ${attempt}/12: waiting for Keycloak operator CRDs to be installed (${missing_crds[*]} still missing)..."
            if [ "${attempt}" -lt 12 ]; then
              sleep 10
            fi
          done

          printf '::error::Keycloak operator CRDs missing after wait: %s\n' "${missing_crds[*]}"
          echo 'kubectl api-resources --api-group=k8s.keycloak.org output:'
          kubectl api-resources --api-group=k8s.keycloak.org || true
          echo 'Existing Keycloak-related CRDs:'
          kubectl get crd | grep -i keycloak || echo 'none'
          echo 'Keycloak pods currently running:'
          kubectl get pods -A | grep -i keycloak || echo 'none'
          echo 'Install the Keycloak operator (https://www.keycloak.org/operator/installation) and rerun the workflow.'
          exit 1

      - name: Wait for platform addons
        shell: bash
        run: |
          set -euo pipefail
          for app in cert-manager cloudnative-pg ingress-nginx; do
            echo "Waiting for Argo CD application ${app}"
            for attempt in $(seq 1 30); do
              sync=$(kubectl -n argocd get application "${app}" -o jsonpath='{.status.sync.status}' 2>/dev/null || true)
              health=$(kubectl -n argocd get application "${app}" -o jsonpath='{.status.health.status}' 2>/dev/null || true)
              phase=$(kubectl -n argocd get application "${app}" -o jsonpath='{.status.operationState.phase}' 2>/dev/null || true)

              if [ "${phase}" = "Failed" ]; then
                echo "${app} last sync failed"
                kubectl -n argocd get application "${app}" -o yaml || true
                exit 1
              fi

              if [ "${health}" = "Healthy" ] && [ "${sync}" = "Synced" ]; then
                echo "${app} is synced and healthy"
                break
              fi

              if [ "${health}" = "Healthy" ] && [ "${sync}" = "OutOfSync" ] && [ "${phase}" = "Succeeded" ]; then
                echo "${app} is healthy but reports sync status OutOfSync (continuing because the last operation succeeded)"
                break
              fi

              if [ "${attempt}" -eq 30 ]; then
                echo "Timed out waiting for ${app} to become healthy"
                kubectl -n argocd get application "${app}" -o yaml || true
                exit 1
              fi

              echo "Attempt ${attempt} for ${app}: sync='${sync}', health='${health}', phase='${phase}'"
              sleep 10
            done
          done

      - name: Ensure ingress load balancer is ready
        shell: bash
        env:
          RESOURCE_GROUP: ${{ inputs.RESOURCE_GROUP }}
          AKS_NAME: ${{ inputs.AKS_NAME }}
        run: |
          set -euo pipefail
          python3 scripts/ensure_ingress_load_balancer.py \
            --ingress-service ingress-nginx/ingress-nginx-controller \
            --resource-group "${RESOURCE_GROUP}" \
            --aks-name "${AKS_NAME}" \
            --timeout "${INGRESS_LB_TIMEOUT:-900}" \
            --interval "${INGRESS_LB_INTERVAL:-15}"

      - name: Configure demo ingress hosts
        id: configure
        shell: bash
        run: |
          set -euo pipefail
          python3 scripts/configure_demo_hosts.py \
            --params-file gitops/apps/iam/params.env \
            --skip-reachability-check

      - name: Force hard recreate of IAM deployment
        shell: bash
        env:
          NAMESPACE_IAM: ${{ inputs.NAMESPACE_IAM }}
        run: |
          set -euo pipefail

          deployment_name="midpoint"

          echo "Ensuring a clean rollout for ${deployment_name} deployment in namespace ${NAMESPACE_IAM}."

          if kubectl -n "${NAMESPACE_IAM}" get deployment "${deployment_name}" >/dev/null 2>&1; then
            echo "Deleting deployment ${deployment_name} in namespace ${NAMESPACE_IAM} to force a hard recreate."
            kubectl -n "${NAMESPACE_IAM}" delete deployment "${deployment_name}" --wait=true
            echo "Deployment ${deployment_name} deleted; Argo CD will recreate it during the next sync."
          else
            echo "Deployment ${deployment_name} does not exist yet; skipping forced recreate."
          fi

      - name: Wait for IAM application
        shell: bash
        run: |
          set -euo pipefail

          max_attempts=6
          sleep_seconds=15
          status="timeout"
          failure_reason="Timed out waiting for IAM application to become healthy."
          operation_message=""

          collect_diagnostics() {
            set +e
            echo '::group::IAM Argo CD application'
            kubectl -n argocd get application iam -o yaml || true
            echo '::endgroup::'
            echo '::group::Keycloak custom resource'
            kubectl -n iam get keycloak rws-keycloak -o yaml || kubectl -n iam get keycloaks.k8s.keycloak.org rws-keycloak -o yaml || true
            echo '::endgroup::'
            echo '::group::Keycloak describe'
            kubectl -n iam describe keycloak rws-keycloak || kubectl -n iam describe keycloaks.k8s.keycloak.org rws-keycloak || true
            echo '::endgroup::'
            echo '::group::IAM namespace pods'
            kubectl -n iam get pods -o wide || true
            echo '::endgroup::'
            echo '::group::Recent IAM namespace events'
            kubectl -n iam get events --sort-by='.metadata.creationTimestamp' | tail -n 40 || true
            echo '::endgroup::'
            echo '::group::Keycloak diagnostics bundle'
            if [ -x scripts/collect_keycloak_diagnostics.sh ]; then
              if ! scripts/collect_keycloak_diagnostics.sh; then
                echo 'collect_keycloak_diagnostics.sh exited with a non-zero status' >&2
              fi
            else
              echo 'collect_keycloak_diagnostics.sh is not present or executable' >&2
            fi
            echo '::endgroup::'
            set -e
          }

          for attempt in $(seq 1 "${max_attempts}"); do
            sync=$(kubectl -n argocd get application iam -o jsonpath='{.status.sync.status}' 2>/dev/null || true)
            health=$(kubectl -n argocd get application iam -o jsonpath='{.status.health.status}' 2>/dev/null || true)
            phase=$(kubectl -n argocd get application iam -o jsonpath='{.status.operationState.phase}' 2>/dev/null || true)
            operation_message=$(kubectl -n argocd get application iam -o jsonpath='{.status.operationState.message}' 2>/dev/null || true)

            echo "Attempt ${attempt} for IAM: sync='${sync}', health='${health}', phase='${phase}'"
            if [ -n "${operation_message}" ]; then
              echo "Last operation message: ${operation_message}"
            fi

            if [ "${sync}" = "Synced" ] && [ "${health}" = "Healthy" ]; then
              echo "IAM application is synced and healthy"
              status="succeeded"
              failure_reason=""
              break
            fi

            if [ "${phase}" = "Failed" ]; then
              echo "IAM application sync failed"
              status="failed"
              failure_reason="IAM application sync failed."
              break
            fi

            if [ "${attempt}" -eq "${max_attempts}" ]; then
              echo "Timed out waiting for IAM application to become healthy"
              break
            fi

            sleep "${sleep_seconds}"
          done

          if [ "${status}" != "succeeded" ]; then
            echo "::warning::IAM application did not reach a healthy state (status=${status}). ${failure_reason}"
            if [ -n "${operation_message}" ]; then
              echo "::notice::Last operation message: ${operation_message}"
            fi
            collect_diagnostics
          fi

          {
            echo "IAM_APP_STATUS=${status}"
            echo "IAM_APP_FAILURE_REASON=${failure_reason}"
            echo "IAM_APP_LAST_OPERATION_MESSAGE=${operation_message}"
          } >>"${GITHUB_ENV}"

      - name: Wait for ingress endpoints to become reachable
        shell: bash
        run: |
          set -euo pipefail

          emit_diagnostics() {
            set +e
            set +u
            set +o pipefail
            echo "::group::Ingress diagnostics"
            echo "📡 Inspecting ingress-nginx service status"
            kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide || true
            kubectl -n ingress-nginx describe svc ingress-nginx-controller || true
            echo "📦 Listing ingress objects"
            kubectl get ingress --all-namespaces || true
            kubectl describe ingress --all-namespaces || true
            echo "📦 Listing cert-manager resources"
            kubectl get certificates --all-namespaces || true
            kubectl get certificaterequests --all-namespaces || true
            kubectl get orders.acme.cert-manager.io --all-namespaces || true
            kubectl describe certificates --all-namespaces || true
            kubectl describe certificaterequests --all-namespaces || true
            kubectl describe orders.acme.cert-manager.io --all-namespaces || true
            echo "🌐 DNS lookup"
            if command -v dig >/dev/null 2>&1; then
              dig +short "${KC_HOST}" || true
            else
              nslookup "${KC_HOST}" || true
            fi
            echo "🔐 TLS handshake information"
            if command -v openssl >/dev/null 2>&1; then
              timeout 15 openssl s_client -connect "${KC_HOST}:443" -servername "${KC_HOST}" < /dev/null || true
            else
              echo "openssl not available on runner"
            fi
            echo "::endgroup::"
            set -e
            set -u
            set -o pipefail
          }

          trap 'emit_diagnostics' ERR

          : "${KC_HOST:?KC_HOST was not exported by configure_demo_hosts.py}"
          : "${MP_HOST:?MP_HOST was not exported by configure_demo_hosts.py}"
          : "${ARGOCD_HOST:?ARGOCD_HOST was not exported by configure_demo_hosts.py}"

          endpoints=(
            "http://${KC_HOST}"
            "http://${MP_HOST}/midpoint"
            "http://${ARGOCD_HOST}"
          )

          max_attempts=${MAX_INGRESS_ATTEMPTS:-10}
          sleep_seconds=${INGRESS_RETRY_DELAY:-30}

          for url in "${endpoints[@]}"; do
            echo "::group::Probing ${url}"
            for attempt in $(seq 1 "${max_attempts}"); do
              if http_code=$(curl --fail --show-error --silent \
                --connect-timeout 5 --max-time 20 --max-redirs 0 --write-out '%{http_code}' \
                --output /dev/null "${url}"); then
                echo "✅ ${url} responded with HTTP ${http_code} (attempt ${attempt}/${max_attempts})."
                break
              fi
              status=$?
              echo "Attempt ${attempt}/${max_attempts} failed for ${url} (exit ${status}); waiting ${sleep_seconds}s before retrying..."
              sleep "${sleep_seconds}"
              if [[ ${attempt} -eq ${max_attempts} ]]; then
                echo "❌ Unable to reach ${url} after ${max_attempts} attempts."
                false
              fi
            done
            echo "::endgroup::"
          done

          trap - ERR

      - name: Commit ingress host parameters
        shell: bash
        run: |
          set -euo pipefail
          KC_HOST_VALUE="${KC_HOST:-}"
          MP_HOST_VALUE="${MP_HOST:-}"
          ARGOCD_HOST_VALUE="${ARGOCD_HOST:-}"
          BRANCH_REF="${GITHUB_REF:-}"
          if git diff --quiet --exit-code; then
            echo "Ingress host parameters already up to date; skipping commit."
          else
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add --update
            git commit -m "Update demo ingress hosts to ${KC_HOST_VALUE}, ${MP_HOST_VALUE}, and ${ARGOCD_HOST_VALUE}"
            if [[ "${BRANCH_REF}" == refs/heads/* ]]; then
              BRANCH_NAME="${BRANCH_REF#refs/heads/}"
              git push origin "HEAD:${BRANCH_NAME}"
            else
              echo "Ref ${BRANCH_REF} is not a branch; skipping push."
            fi
          fi

      - name: Summary
        shell: bash
        run: |
          set -euo pipefail
          iam_status="${IAM_APP_STATUS:-unknown}"
          iam_reason="${IAM_APP_FAILURE_REASON:-}"
          iam_message="${IAM_APP_LAST_OPERATION_MESSAGE:-}"

          if [ "${iam_status}" = "succeeded" ]; then
            echo "✅ Argo CD and IAM workloads are converged."
          else
            echo "⚠️ IAM workloads did not converge (status=${iam_status})."
            if [ -n "${iam_reason}" ]; then
              echo "ℹ️ ${iam_reason}"
            fi
            if [ -n "${iam_message}" ]; then
              echo "ℹ️ Last operation message: ${iam_message}"
            fi
          fi

          echo "✅ Demo ingress hosts are configured."
          echo "🔗 Keycloak : ${{ steps.configure.outputs.keycloak_url }}"
          echo "🔗 midPoint : ${{ steps.configure.outputs.midpoint_url }}"
          echo "🔗 Argo CD  : ${{ steps.configure.outputs.argocd_url }}"
          echo "🌐 External IP : ${EXTERNAL_IP:-unknown}"
